{"index":{"version":"0.5.12","fields":[{"name":"title","boost":10},{"name":"keywords","boost":15},{"name":"body","boost":1}],"ref":"url","documentStore":{"store":{"./":["(knowledg","61a).","along","appli","berkeley'","best","broad,","compar","compil","cs","dozen","evolv","exercis","experi","field,","gone","guidebook","guidebook,","guidelin","help","here.","i'v","insert","intimid","introduct","learn","machin","make","maxim","newcomer.","note","possibl","pretti","prior","process","program","python","rapidli","recommend","resourc","roughli","scrap","seamless","skill","slew","someon","through","togeth","tutorials,","us","want","way.","who'","you."],"the-basics/overview.html":["basic"],"the-basics/what-is-ml.html":["learning?","machin"],"the-basics/types-of-ml.html":["learn","machin","type"],"the-basics/ml-pipeline.html":["learn","machin","pipelin"],"the-basics/software-tools.html":["softwar","tool"],"reinforcement-learning/overview.html":["action","agent","area","deal","environ","learn","machin","maxim","order","reinforc","reward.","take"],"reinforcement-learning/bellman-eqn.html":["&=","(","(click","(r_{t+2}",")",")∣st=s]=eπ[rt+1+γgt+1∣st=s]=eπ[rt+1+γvπ(st+1)∣st=s]\\begin{align}","+","188:","2022","8","=","\\\\","\\cdot","\\cdots)","\\end{align}vπ​(s)​=eπ​[gt​∣st​=s]=eπ​[rt+1​+γrt+2​+γ2rt+3​+⋯∣st​=s]=eπ​[rt+1​+γ(rt+2​+γrt+3​+⋯)∣st​=s]=eπ​[rt+1​+γgt+1​∣st​=s]=eπ​[rt+1​+γvπ​(st+1​)∣st​=s]​​","\\gamma","\\gamma^2","\\in","\\mathcal{p}_{ss'}^a","\\mathcal{r}_s^a","\\pi(a'|s')","\\pi(a|s)","\\sum_{a","\\sum_{a'","\\sum_{a\\in","\\sum_{s'","a')","a)","a)vπ​(s)=a∈a∑​π(a∣s)qπ​(s,a)","a]qπ​(s,a)=eπ​[rt+1​+γqπ​(st+1​,at+1​)∣st​=s,at​=a]","a_t","a_{t+1})","above,","action","action,","action.","adapt","ahead","artifici","as:","associ","a}","bellman","berkeley'","between","closer","combin","consid","convers","cs","current","david","decis","decompos","discuss","do","e_\\pi[g_t","e_\\pi[r_{t+1}","each","end","equat","expect","express","find","follows:","function","function,","function:","g_{t+1}","given","good","immedi","intelligence,","introduct","it'","it.","last","let'","look","markov","mathematically,","matrix.","me)","new","next","note","now","now,","one.","perform","pictori","possibl","process","q","q_\\pi(s',","q_\\pi(s,","q_\\pi(s,a)","q_\\pi(s_{t+1},","qqq","qπ(s,a)=eπ[rt+1+γqπ(st+1,at+1)∣st=s,at=a]q_\\pi(s,","qπ(s,a)=rsa+γ∑s′∈spss′avπ(s′)","qπ(s,a)=rsa+γ∑s′∈spss′a∑a′∈aπ(a′∣s′)qπ(s′,a′)","qπ​(s,a)=rsa​+γs′∈s∑​pss′a​a′∈a∑​π(a′∣s′)qπ​(s′,a′)","qπ​(s,a)=rsa​+γs′∈s∑​pss′a​vπ​(s′)","r_{t+2}","r_{t+3}","realli","recurr","recurs","relat","relationship","relationship,","represent","return","reward","s,","s]","s_t","same","see","silver'","similarly,","specif","spring","state","state,","state.","structure.","sum","s}","take","take,","those","time.","transit","two","uc","up","us","v_\\pi(s')","v_\\pi(s)","v_\\pi(s_{t+1})","valu","vvv","vπ(s)=eπ[gt∣st=s]=eπ[rt+1+γrt+2+γ2rt+3+⋯∣st=s]=eπ[rt+1+γ(rt+2+γrt+3+⋯","vπ(s)=∑a∈aπ(a∣s)=(rsa+γ∑s′∈spss′avπ(s′))","vπ(s)=∑a∈aπ(a∣s)qπ(s,a)v_\\pi(s)","vπ​(s)=a∈a∑​π(a∣s)=(rsa​+γs′∈s∑​pss′a​vπ​(s′))","we'r","weight","wonder","|","💡"],"reinforcement-learning/policies-values.html":["\"how","1.","188:","2022","8","=","[","\\displaystyle\\sum_{k=0}^\\infti","\\gamma^k","\\pi(s)a=π(s)","\\rightarrow","a)","a=π(s)a","a]qπ​(s,a)=eπ​[gt​∣st​=s,at​=a]","a_t","aaa","action","action.","adapt","after.","agent","agent'","articl","artifici","aπ:s→a,","behavior.","berkeley'","call","choos","compar","cs","deep","describ","determinist","discuss","distribut","e_\\pi","e_\\pi[g_t","each","equal","estim","evalu","expect","follow","function","function,","futur","g_t","given","good","good\"","goodness/bad","greater","gt=∑k=0∞γkrt+k+1g_t","intelligence,","introduct","known","lizard'","map","measur","note","now,","optim","output","over","p[a_t=a|s_t=s]π(a∣s)=p[at​=a∣st​=s]","pair","polici","policies.","policy,","policy:","predict","q","qualiti","qπ(s,a)=eπ[gt∣st=s,at=a]q_\\pi(s,","qπq_\\piqπ​","r_{t+k+1}gt​=k=0∑∞​γkrt+k+1​","recall,","repres","return","reward","s","s,","s]vπ​(s)=eπ​[gt​∣st​=s]","s_t","specif","spring","sss","start","state","state.","states.","stochast","take","thereafter.1","time","ttt","uc","us","util","valu","value,","vπ(s)=eπ[gt∣st=s]v_\\pi(s)","vπv_\\pivπ​","|","π(a∣s)=p[at=a∣st=s]\\pi(a|s)","π:s→a\\pi:","π\\piπ","π∗\\pi^*π∗","↩"],"reinforcement-learning/optimality.html":["&\\text{if","&\\text{otherwise}","(s),","(s)v∗​(s)=πmax​vπ​(s)","(s,","+","0","1","=","\\\\","\\begin{cases}","\\end{cases}","\\foral","\\gamma","\\geq","\\in","\\mathcal{p}_{ss'}^a","\\mathcal{r}_s^a","\\mathcal{r}_x^a","\\max_\\pi","\\pi'","\\pi,","\\pi_*(a|s)","\\piπ∗​≥π,∀π","\\sum_{s'","\\text{","\\underset{a","\\underset{a'}{\\max}","\\underset{a}{\\max}","a')q∗​(s,a)=rsa​+γs′∈s∑​pss′a​a′max​","a)","a)qπ∗​​(s,a)=q∗​(s,a)","a)q∗​(s,a)","a)q∗​(s,a)=πmax​qπ​(s,a)","a)v∗​(s)=amax​","a=arg⁡max⁡a∈a","a=a∈aargmax​","achiev","action","action,","adapt","ahead","algorithm","alway","and,","another.","a}{\\arg\\max}","befor","begin","bellman","best","better","both","but,","choos","consid","current","david","decis","defin","describ","determinist","differ","each","effect","end","equal","equat","equation.","exist","expect","find","first","follow","function","function,","function.","given","great","greatest","holds:","immedi","introduc","intuitively,","know","look","make","markov","max","maximum","mdp","mdp!","mdp,","mdp.","multipli","notion","now","onc","one:","optim","optimality,","outlin","over","particular","perform","polici","policies:","possibl","probabl","process","q","q_*(s',","q_*(s,","q_\\pi(s,","question.","qπ∗(s,a)=q∗(s,a)q_{\\pi_*}(s,","q∗(s,a)0otherwis","q∗(s,a)=max⁡πqπ(s,a)q_*(s,a)","q∗(s,a)=rsa+γ∑s′∈spss′amax⁡a′","q∗(s,a)=rxa+γ∑s′∈spss′av∗(s′)","q∗(s,a)q_*(s,","q∗(s,a)v_*(s)","q∗(s′,a′)q_*(s,","q∗q_*q∗​","q∗q_*q∗​,","q∗​(s,a)","q∗​(s,a)=rxa​+γs′∈s∑​pss′a​v∗​(s′)","q∗​(s,a)otherwise​","q∗​(s′,a′)","recal","recurs","relationship","relationship:","return—which","reward","rsa+γ∑s′∈spss′av∗(s′)v_*(s)","rsa​+γs′∈s∑​pss′a​v∗​(s′)","s","saying,","silver'","solv","start","state","state,","state.","sum","s}","sπ≥π′","take","take,","that'","that,","theorem","up","v_*(s')","v_*(s)vπ∗​​(s)=v∗​(s)","v_\\pi","v_\\pi(s)","v_{\\pi'}","valu","values.","vπ(s)≥vπ′(s),∀s∈s\\pi","vπ​(s)≥vπ′​(s),∀s∈","vπ∗(s)=v∗(s)v_{\\pi_*}(s)","v∗(s)=max⁡a","v∗(s)=max⁡πvπ(s)v_*(s)","v∗(s)v_*(s)v∗​(s)","v∗v_*v∗​,","v∗v_*v∗​:","v∗​(s)=amax​","wait...how","we'll","with?","wondering,","wow!","yield","}","}q_*","π\\piπ","π′\\pi'π′","π∗(a∣s)={1if","π∗\\pi_*π∗​","π∗​(a∣s)=⎩⎨⎧​10​if","π∗≥π,∀π\\pi_*","π≥π′","💡"],"reinforcement-learning/mdp.html":["\"lifetime\"","&","(click","(each","(immediate)","(mdps)","(γ\\gammaγ","+","0","0,","1","1)","188:","2022","8","=","=e[r_{t+1}","\\\\","\\begin{bmatrix}","\\cdot","\\cdotss0​a0​​s1​a1​​s2​a2​​⋯","\\displaystyle\\sum_{k=0}^\\infti","\\displaystyle\\sum_{t=0}^\\infti","\\end{bmatrix}top=from⎣⎡​p11​pn1​​⋯⋮⋯​p1n​pnn​​⎦⎤​","\\frac{r_{max}}{1","\\gamma","\\gamma^k","\\gamma^t","\\gamma}gt​=rt+1​+γrt+1​+⋯=k=0∑∞​γkrt+k+1​≤t=0∑∞​γtrmax​=1−γrmax​​","\\in","\\leq","\\mathcal{p}","\\mathcal{p}_{s,s'}^a","\\mathcal{r}_s^a","\\text{from}","\\text{to}","\\vdot","\\xrightarrow{a_0}","\\xrightarrow{a_1}","\\xrightarrow{a_2}","a)","a,","a]ps,s′a​=p[st+1​=s′∣st​=s,at​=a]","a]rsa​=e[rt+1​∣st​=s,at​=a]","a_t","aaa","aa∈a","action","action.","adapt","affect","agent","agent'","animal/human","arriv","arrives,","artifici","as:","ata_tat​.","avoid","a∈aa","befor","behavior","berkeley'","between","card","choos","close","compon","condition","construct","conveni","cs","cumul","current","cyclic","deal","decid","decis","defin","degre","depend","describ","discount","discret","do","end","environ","equal","etc.","evaluation).","evaluation;","expect","express","factor","factors.","far","feedback","finit","follow","fulli","function","futur","game","given","goal","grow","gt=rt+1+rt+1+⋯=∑k=0∞rt+k+1g_t","gt=rt+1+γrt+1+⋯=∑k=0∞γkrt+k+1≤∑t=0∞γtrmax=rmax1−γg_t","gtg_tgt​","horizon","however,","hypothesi","idea","immedi","independ","independent.","indic","infinit","inher","intelligence,","introduc","introduct","keep","lead","limit","live","longer","maker","markov","mathemat","mathematically,","matrix","maxim","mdp","me)","mean","memoryless","memoryless,","model:","more","multipl","myopic","next","nondeterminist","note","now","now,","number","onc","optim","p(s'","p[s_{t+1}","p[st+1∣st]=p[st+1∣s1,⋯st]p[s_{t+1}","p\\mathcal{p}p","p_{11}","p_{1n}","p_{n1}","p_{nn}","past","perform","place","player","poker","possibl","predict","prefer","present","present.","previou","probabl","problem","problems,","process","processes,","processes.","properti","ps,s′a=p[st+1=s′∣st=s,at=a]","r_{max}","r_{t+1}","r_{t+k+1}","r_{t+k+1}gt​=rt+1​+rt+1​+⋯=k=0∑∞​rt+k+1​","random","rate","rate,","repres","represent","represented,","return","reward","reward:","rewards,","rewards?","row","rsa=e[rt+1∣st=s,at=a]","rtr_trt​","s'","s')","s,","s0→a0s1→a1s2→a2⋯s_0","s_1","s_1,","s_2","s_t","s_t]","s_t]p[st+1​∣st​]=p[st+1​∣s1​,⋯st​]","same","scalar","search","seem","set","show","sight","signal","singl","solv","spring","sss","ss∈","start","state","state,","states.","sts_tst​","succeed","successor","sum","s′s's′","s∈ss","t(s,a,s′)=p(s′∣s,a).","t(s,a,s′)=p(s′∣s,a)t(s,","t+1t+1t+1","take","take.","termin","terminated.","think","through","time","timestep","top=from[p11⋯p1n⋮pn1⋯pnn]","total","transit","ttt","ttt,","ttt:","uc","unboundedly.","uncertainti","up","us","util","valu","view","well","words,","|","γ\\gammaγ","γ\\gammaγ,","→","🎲","💡"]},"length":11},"tokenStore":{"root":{"0":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"1":{"8":{"8":{"docs":{},":":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}},"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},")":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"2":{"0":{"2":{"2":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{}},"docs":{}},"docs":{}},"6":{"1":{"docs":{},"a":{"docs":{},")":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"docs":{}},"8":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{},"(":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"k":{"docs":{},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{},"l":{"docs":{},"e":{"docs":{},"d":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}}}},"c":{"docs":{},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{},"k":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"r":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"docs":{}}}}}},"s":{"docs":{},")":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},"v":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"π":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"​":{"docs":{},"v":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"e":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"i":{"docs":{},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},")":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"m":{"docs":{},"d":{"docs":{},"p":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":2.5023364485981308}}}}}}},"γ":{"docs":{},"\\":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"γ":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.031746031746031744}}}}},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"w":{"docs":{},"a":{"docs":{},"y":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"p":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.042328042328042326},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.016611295681063124},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.016355140186915886}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"h":{"docs":{},"i":{"docs":{},"e":{"docs":{},"v":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.02336448598130841}},"'":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667}}}},"t":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"c":{"docs":{},"l":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"r":{"docs":{},"i":{"docs":{},"v":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"'":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"q":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"​":{"docs":{},"a":{"docs":{},"′":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"​":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.013289036544850499},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"v":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"∈":{"docs":{},"a":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"π":{"docs":{},"(":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},")":{"docs":{},"q":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"​":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}},"q":{"docs":{},"π":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"q":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"=":{"docs":{},"π":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"​":{"docs":{},"q":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}},"]":{"docs":{},"q":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"​":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"q":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},")":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}},"docs":{}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}},"g":{"docs":{},"t":{"docs":{},"​":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"s":{"docs":{},",":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"​":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"​":{"docs":{},"=":{"docs":{},"e":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"docs":{}}}}},"b":{"docs":{},"o":{"docs":{},"v":{"docs":{},"e":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"d":{"docs":{},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"h":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"s":{"docs":{},":":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875}},"{":{"docs":{},"\\":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"\\":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"=":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"a":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"⁡":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"⁡":{"docs":{},"a":{"docs":{},"∈":{"docs":{},"a":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}},"∈":{"docs":{},"a":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"​":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"a":{"docs":{},"a":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"∈":{"docs":{},"a":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"f":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"π":{"docs":{},":":{"docs":{},"s":{"docs":{},"→":{"docs":{},"a":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}},"n":{"docs":{},"d":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{},"/":{"docs":{},"h":{"docs":{},"u":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"t":{"docs":{},"a":{"docs":{},"_":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"​":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"v":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"∈":{"docs":{},"a":{"docs":{},"a":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"k":{"docs":{},"e":{"docs":{},"l":{"docs":{},"e":{"docs":{},"y":{"docs":{},"'":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"l":{"docs":{},"l":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":5.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}},"t":{"docs":{},"w":{"docs":{},"e":{"docs":{},"e":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}},"h":{"docs":{},"a":{"docs":{},"v":{"docs":{},"i":{"docs":{},"o":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"r":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"a":{"docs":{},"s":{"docs":{},"i":{"docs":{},"c":{"docs":{"the-basics/overview.html":{"ref":"the-basics/overview.html","tf":10}}}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"u":{"docs":{},"t":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"i":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"b":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"n":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"d":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"s":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"l":{"docs":{},"o":{"docs":{},"s":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"r":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}},"m":{"docs":{},"u":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"r":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"h":{"docs":{},"o":{"docs":{},"o":{"docs":{},"s":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"y":{"docs":{},"c":{"docs":{},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"d":{"docs":{},"o":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"z":{"docs":{},"e":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"e":{"docs":{},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":2.5070093457943927}}},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}},"e":{"docs":{},"p":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"s":{"docs":{},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},"b":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}},"f":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"a":{"docs":{},"v":{"docs":{},"i":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"i":{"docs":{},"s":{"docs":{},"c":{"docs":{},"u":{"docs":{},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.018691588785046728}}}}}},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"b":{"docs":{},"u":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"e":{"docs":{},"v":{"docs":{},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},")":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},";":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"x":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.031746031746031744}}}},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.021164021164021163},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"n":{"docs":{},"v":{"docs":{},"i":{"docs":{},"r":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}}},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}},"[":{"docs":{},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"r":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125}}}},"docs":{}}}}}}}}}}},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":5.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"l":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"t":{"docs":{},"c":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"l":{"docs":{},"d":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"n":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}},"i":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"o":{"docs":{},"l":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"s":{"docs":{},":":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.015625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":3.386243386243386},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":2.526578073089701},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}},":":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.014018691588785047}}}}},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"s":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"e":{"docs":{},"e":{"docs":{},"d":{"docs":{},"b":{"docs":{},"a":{"docs":{},"c":{"docs":{},"k":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"g":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"o":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581}},"\"":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"n":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"/":{"docs":{},"b":{"docs":{},"a":{"docs":{},"d":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"u":{"docs":{},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}},",":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}}},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"docs":{}}}},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.026455026455026454},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"t":{"docs":{},"=":{"docs":{},"∑":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"∞":{"docs":{},"γ":{"docs":{},"k":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"docs":{}}}}}}}}}},"docs":{}}}},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"⋯":{"docs":{},"=":{"docs":{},"∑":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"∞":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}},"docs":{}}}},"γ":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"⋯":{"docs":{},"=":{"docs":{},"∑":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"∞":{"docs":{},"γ":{"docs":{},"k":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"≤":{"docs":{},"∑":{"docs":{},"t":{"docs":{},"=":{"0":{"docs":{},"∞":{"docs":{},"γ":{"docs":{},"t":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"=":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"1":{"docs":{},"−":{"docs":{},"γ":{"docs":{},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}},"docs":{}}}}},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{},"g":{"docs":{},"t":{"docs":{},"​":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"h":{"docs":{},"e":{"docs":{},"l":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"r":{"docs":{},"e":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"s":{"docs":{},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"r":{"docs":{},"i":{"docs":{},"z":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}},"w":{"docs":{},"e":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"y":{"docs":{},"p":{"docs":{},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"s":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"i":{"docs":{},"'":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"n":{"docs":{},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"i":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"r":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"t":{"docs":{"./":{"ref":"./","tf":10.015873015873016},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"u":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"i":{"docs":{},"c":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"f":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"t":{"docs":{},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"d":{"docs":{},"e":{"docs":{},"a":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.07936507936507936},"the-basics/types-of-ml.html":{"ref":"the-basics/types-of-ml.html","tf":3.6666666666666665},"the-basics/ml-pipeline.html":{"ref":"the-basics/ml-pipeline.html","tf":3.6666666666666665},"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":5.2}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"?":{"docs":{"the-basics/what-is-ml.html":{"ref":"the-basics/what-is-ml.html","tf":5.5}}}}}}}},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}},"t":{"docs":{},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"n":{"docs":{},"g":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"i":{"docs":{},"z":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"'":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"v":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"m":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.047619047619047616},"the-basics/what-is-ml.html":{"ref":"the-basics/what-is-ml.html","tf":5.5},"the-basics/types-of-ml.html":{"ref":"the-basics/types-of-ml.html","tf":3.6666666666666665},"the-basics/ml-pipeline.html":{"ref":"the-basics/ml-pipeline.html","tf":3.6666666666666665},"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667}}}}}},"k":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"x":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"i":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},"u":{"docs":{},"m":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}},"r":{"docs":{},"k":{"docs":{},"o":{"docs":{},"v":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":2.5093457943925235}}}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"i":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"p":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"e":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"a":{"docs":{},"s":{"docs":{},"u":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"m":{"docs":{},"o":{"docs":{},"r":{"docs":{},"y":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"d":{"docs":{},"p":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"!":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{},"p":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"i":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},":":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"r":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"y":{"docs":{},"o":{"docs":{},"p":{"docs":{},"i":{"docs":{},"c":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}}},"x":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"o":{"docs":{},"t":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"w":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}}}}}}}}},"u":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":3.386243386243386},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":2.5299003322259135}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}},"y":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},":":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581}}}}}}},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{},"t":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"v":{"docs":{},"i":{"docs":{},"o":{"docs":{},"u":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"i":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.031746031746031744},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":2.5023364485981308}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"b":{"docs":{},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.011682242990654205}}}}},"l":{"docs":{},"e":{"docs":{},"m":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}}},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"i":{"docs":{},"p":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"the-basics/ml-pipeline.html":{"ref":"the-basics/ml-pipeline.html","tf":3.6666666666666665}}}}}}},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}},"e":{"docs":{},"r":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"[":{"docs":{},"a":{"docs":{},"_":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"|":{"docs":{},"s":{"docs":{},"_":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"π":{"docs":{},"(":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"a":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}},"docs":{}}}}},"t":{"docs":{},"+":{"1":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"]":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"∣":{"docs":{},"s":{"1":{"docs":{},",":{"docs":{},"⋯":{"docs":{},"s":{"docs":{},"t":{"docs":{},"]":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}},"a":{"docs":{},"i":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"u":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"\\":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"{":{"docs":{},"p":{"docs":{},"}":{"docs":{},"p":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}},"_":{"docs":{},"{":{"1":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{},"n":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"docs":{},"n":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{},"n":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"l":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"y":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"s":{"docs":{},",":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"=":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{},"i":{"docs":{},"d":{"docs":{},"l":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"t":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"e":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"l":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}},"s":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"c":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"c":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":5.133333333333334}}}}}}}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.02336448598130841}},".":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667}}},":":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}},"?":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}},"l":{"docs":{},"a":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},"h":{"docs":{},"i":{"docs":{},"p":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.021164021164021163},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},"—":{"docs":{},"w":{"docs":{},"h":{"docs":{},"i":{"docs":{},"c":{"docs":{},"h":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"o":{"docs":{},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{},"l":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"w":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.009345794392523364}}}},"2":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"3":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125}}}},"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"g":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"∑":{"docs":{},"∞":{"docs":{},"​":{"docs":{},"γ":{"docs":{},"k":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"docs":{}}}}}}}}}}}},"docs":{}}},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"⋯":{"docs":{},"=":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"∑":{"docs":{},"∞":{"docs":{},"​":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"s":{"docs":{},"a":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"∑":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"v":{"docs":{},"∗":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}}},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"​":{"docs":{},"v":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}},"=":{"docs":{},"e":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}},"docs":{}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"t":{"docs":{},"​":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"s":{"0":{"docs":{},"→":{"docs":{},"a":{"0":{"docs":{},"s":{"1":{"docs":{},"→":{"docs":{},"a":{"1":{"docs":{},"s":{"2":{"docs":{},"→":{"docs":{},"a":{"2":{"docs":{},"⋯":{"docs":{},"s":{"docs":{},"_":{"0":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{}}}}},"docs":{}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}}}},"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"c":{"docs":{},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"a":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"e":{"docs":{},"a":{"docs":{},"m":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"e":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"m":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}},"k":{"docs":{},"i":{"docs":{},"l":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"l":{"docs":{},"e":{"docs":{},"w":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"o":{"docs":{},"m":{"docs":{},"e":{"docs":{},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"f":{"docs":{},"t":{"docs":{},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{"the-basics/software-tools.html":{"ref":"the-basics/software-tools.html","tf":5.5}}}}}}},"l":{"docs":{},"v":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}},"]":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125}},"v":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"​":{"docs":{},"[":{"docs":{},"g":{"docs":{},"t":{"docs":{},"​":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}},"_":{"1":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"2":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0234375},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"]":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"∣":{"docs":{},"s":{"1":{"docs":{},"​":{"docs":{},",":{"docs":{},"⋯":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}}}}}}},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"y":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"i":{"docs":{},"l":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"m":{"docs":{},"i":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"n":{"docs":{},"g":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"f":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872}}}}}},"r":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.015625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.05291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.04439252336448598}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},"s":{"docs":{},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"r":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"r":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}},"s":{"docs":{},"_":{"docs":{},"t":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"u":{"docs":{},"m":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"c":{"docs":{},"c":{"docs":{},"e":{"docs":{},"e":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}}}},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}},"∈":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"π":{"docs":{},"≥":{"docs":{},"π":{"docs":{},"′":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"'":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},")":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"′":{"docs":{},"s":{"docs":{},"'":{"docs":{},"s":{"docs":{},"′":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}}},"∈":{"docs":{},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"o":{"docs":{},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"o":{"docs":{},"s":{"docs":{},"e":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"f":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"1":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"docs":{}}}}}}}}},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"m":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"a":{"docs":{},"t":{"docs":{},"'":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"i":{"docs":{},"n":{"docs":{},"k":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"o":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"o":{"docs":{},"l":{"docs":{"the-basics/software-tools.html":{"ref":"the-basics/software-tools.html","tf":5.5}}}},"p":{"docs":{},"=":{"docs":{},"f":{"docs":{},"r":{"docs":{},"o":{"docs":{},"m":{"docs":{},"[":{"docs":{},"p":{"1":{"1":{"docs":{},"⋯":{"docs":{},"p":{"1":{"docs":{},"n":{"docs":{},"⋮":{"docs":{},"p":{"docs":{},"n":{"1":{"docs":{},"⋯":{"docs":{},"p":{"docs":{},"n":{"docs":{},"n":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"docs":{}}}}}},"docs":{}}}},"docs":{}},"docs":{}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}}}}},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{"the-basics/types-of-ml.html":{"ref":"the-basics/types-of-ml.html","tf":3.6666666666666665}}}}},"a":{"docs":{},"k":{"docs":{},"e":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"p":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.014018691588785047}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}}}},"w":{"docs":{},"o":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"t":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},":":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},",":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{},"=":{"docs":{},"p":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∣":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"t":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}}}}}}}}}},"+":{"1":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{}}}},"docs":{}}}},"docs":{}},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"d":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}},"u":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"c":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"p":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"t":{"docs":{},"i":{"docs":{},"l":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"n":{"docs":{},"b":{"docs":{},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"d":{"docs":{},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"c":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}}}}}}}}},"w":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"y":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"i":{"docs":{},"t":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}},"h":{"docs":{},"o":{"docs":{},"'":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"e":{"docs":{},"'":{"docs":{},"r":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"l":{"docs":{},"l":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}},"l":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"o":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"w":{"docs":{},"!":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"?":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"y":{"docs":{},"o":{"docs":{},"u":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"i":{"docs":{},"e":{"docs":{},"l":{"docs":{},"d":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667}}}}}},"n":{"docs":{},"e":{"docs":{},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"c":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":2.553156146179402},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}},"u":{"docs":{},"t":{"docs":{},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}},"&":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.014018691588785047}},"=":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125}}},"\\":{"docs":{},"t":{"docs":{},"e":{"docs":{},"x":{"docs":{},"t":{"docs":{},"{":{"docs":{},"i":{"docs":{},"f":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"w":{"docs":{},"i":{"docs":{},"s":{"docs":{},"e":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"g":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"v":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},")":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"\\":{"docs":{},"b":{"docs":{},"e":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"{":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}}}}}},"docs":{}}}}}}}}}}}}}}},"+":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.046875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.009345794392523364}}},"=":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.05078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.042328042328042326},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.03322259136212625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.03271028037383177}},"e":{"docs":{},"[":{"docs":{},"r":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}}}}}}}}},"\\":{"docs":{},"\\":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.009345794392523364}}},"c":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.011682242990654205}},"s":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"s":{"0":{"docs":{},"​":{"docs":{},"a":{"0":{"docs":{},"​":{"docs":{},"​":{"docs":{},"s":{"1":{"docs":{},"​":{"docs":{},"a":{"1":{"docs":{},"​":{"docs":{},"​":{"docs":{},"s":{"2":{"docs":{},"​":{"docs":{},"a":{"2":{"docs":{},"​":{"docs":{},"​":{"docs":{},"⋯":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"docs":{}}}},"docs":{}}}}},"docs":{}}}},"docs":{}}}}},"docs":{}}}},"docs":{}}}}}}},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"{":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{},"}":{"docs":{},"v":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"​":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"​":{"docs":{},"[":{"docs":{},"g":{"docs":{},"t":{"docs":{},"​":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"​":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"2":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"3":{"docs":{},"​":{"docs":{},"+":{"docs":{},"⋯":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"​":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"(":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"3":{"docs":{},"​":{"docs":{},"+":{"docs":{},"⋯":{"docs":{},")":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"​":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"g":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"​":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"v":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},")":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"​":{"docs":{},"​":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}}}}},"docs":{}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"s":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"b":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{},"}":{"docs":{},"t":{"docs":{},"o":{"docs":{},"p":{"docs":{},"=":{"docs":{},"f":{"docs":{},"r":{"docs":{},"o":{"docs":{},"m":{"docs":{},"⎣":{"docs":{},"⎡":{"docs":{},"​":{"docs":{},"p":{"1":{"1":{"docs":{},"​":{"docs":{},"p":{"docs":{},"n":{"1":{"docs":{},"​":{"docs":{},"​":{"docs":{},"⋯":{"docs":{},"⋮":{"docs":{},"⋯":{"docs":{},"​":{"docs":{},"p":{"1":{"docs":{},"n":{"docs":{},"​":{"docs":{},"p":{"docs":{},"n":{"docs":{},"n":{"docs":{},"​":{"docs":{},"​":{"docs":{},"⎦":{"docs":{},"⎤":{"docs":{},"​":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.03515625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"^":{"2":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"docs":{},"k":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"}":{"docs":{},"g":{"docs":{},"t":{"docs":{},"​":{"docs":{},"=":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"+":{"docs":{},"⋯":{"docs":{},"=":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"∑":{"docs":{},"∞":{"docs":{},"​":{"docs":{},"γ":{"docs":{},"k":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"​":{"docs":{},"≤":{"docs":{},"t":{"docs":{},"=":{"0":{"docs":{},"∑":{"docs":{},"∞":{"docs":{},"​":{"docs":{},"γ":{"docs":{},"t":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"​":{"docs":{},"=":{"1":{"docs":{},"−":{"docs":{},"γ":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"​":{"docs":{},"​":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"docs":{}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}},"e":{"docs":{},"q":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.016611295681063124},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"{":{"docs":{},"p":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"_":{"docs":{},"{":{"docs":{},"s":{"docs":{},"s":{"docs":{},"'":{"docs":{},"}":{"docs":{},"^":{"docs":{},"a":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}}},",":{"docs":{},"s":{"docs":{},"'":{"docs":{},"}":{"docs":{},"^":{"docs":{},"a":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}},"r":{"docs":{},"}":{"docs":{},"_":{"docs":{},"s":{"docs":{},"^":{"docs":{},"a":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"x":{"docs":{},"^":{"docs":{},"a":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}},"x":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"a":{"docs":{},"'":{"docs":{},"|":{"docs":{},"s":{"docs":{},"'":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"|":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125}}}}}},"s":{"docs":{},")":{"docs":{},"a":{"docs":{},"=":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}},"'":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"a":{"docs":{},"|":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"π":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"≥":{"docs":{},"π":{"docs":{},",":{"docs":{},"∀":{"docs":{},"π":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"_":{"docs":{},"{":{"docs":{},"a":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"\\":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}},"s":{"docs":{},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"p":{"docs":{},"l":{"docs":{},"a":{"docs":{},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"y":{"docs":{},"l":{"docs":{},"e":{"docs":{},"\\":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"_":{"docs":{},"{":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"}":{"docs":{},"^":{"docs":{},"\\":{"docs":{},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}}}}},"docs":{}}},"t":{"docs":{},"=":{"0":{"docs":{},"}":{"docs":{},"^":{"docs":{},"\\":{"docs":{},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}},"b":{"docs":{},"e":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"{":{"docs":{},"c":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"s":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"b":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"{":{"docs":{},"r":{"docs":{},"_":{"docs":{},"{":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{},"}":{"docs":{},"{":{"1":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{}}}}}}}}}}}}}}},"t":{"docs":{},"e":{"docs":{},"x":{"docs":{},"t":{"docs":{},"{":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.016611295681063124}},"f":{"docs":{},"r":{"docs":{},"o":{"docs":{},"m":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"t":{"docs":{},"o":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{},"{":{"docs":{},"a":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"'":{"docs":{},"}":{"docs":{},"{":{"docs":{},"\\":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"}":{"docs":{},"{":{"docs":{},"\\":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"q":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"v":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"x":{"docs":{},"r":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"w":{"docs":{},"{":{"docs":{},"a":{"docs":{},"_":{"0":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"2":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}}}}}}}}}}}}}}}},"q":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"a":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"docs":{}}}}}}}}}},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}}},"q":{"docs":{},"q":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"q":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},")":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{},"q":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}},"docs":{}}}}}}}}}},"docs":{}}}},"g":{"docs":{},"t":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{},"q":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"∑":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"v":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}},"∑":{"docs":{},"a":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"a":{"docs":{},"π":{"docs":{},"(":{"docs":{},"a":{"docs":{},"′":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{},"q":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},",":{"docs":{},"a":{"docs":{},"′":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"​":{"docs":{},"a":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"a":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"π":{"docs":{},"(":{"docs":{},"a":{"docs":{},"′":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{},"q":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},",":{"docs":{},"a":{"docs":{},"′":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"q":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"q":{"docs":{},"π":{"docs":{},"​":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}},"∗":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"q":{"docs":{},"∗":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"q":{"docs":{},"_":{"docs":{},"{":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"_":{"docs":{},"*":{"docs":{},"}":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"∗":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"0":{"docs":{},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"w":{"docs":{},"i":{"docs":{},"s":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}},"docs":{},"=":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"⁡":{"docs":{},"π":{"docs":{},"q":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"∑":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"⁡":{"docs":{},"a":{"docs":{},"′":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}},"x":{"docs":{},"a":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"∑":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"v":{"docs":{},"∗":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}},"′":{"docs":{},",":{"docs":{},"a":{"docs":{},"′":{"docs":{},")":{"docs":{},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{},"q":{"docs":{},"∗":{"docs":{},"​":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"=":{"docs":{},"r":{"docs":{},"x":{"docs":{},"a":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"​":{"docs":{},"v":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"w":{"docs":{},"i":{"docs":{},"s":{"docs":{},"e":{"docs":{},"​":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}},"′":{"docs":{},",":{"docs":{},"a":{"docs":{},"′":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}},"v":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125}}}},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"docs":{}}}}}}}}}},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}},")":{"docs":{},"v":{"docs":{},"π":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"v":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}},"{":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"'":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":3.3703703703703702},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":2.5332225913621262},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"e":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"s":{"docs":{},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"v":{"docs":{},"v":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"[":{"docs":{},"g":{"docs":{},"t":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"+":{"docs":{},"γ":{"2":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"3":{"docs":{},"+":{"docs":{},"⋯":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"π":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"(":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"3":{"docs":{},"+":{"docs":{},"⋯":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"docs":{}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}},"docs":{}}}},"docs":{}}}}}}},"docs":{}}}}}}}},"v":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}},"∑":{"docs":{},"a":{"docs":{},"∈":{"docs":{},"a":{"docs":{},"π":{"docs":{},"(":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"(":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"∑":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"v":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}},"q":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"v":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}},"≥":{"docs":{},"v":{"docs":{},"π":{"docs":{},"′":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},",":{"docs":{},"∀":{"docs":{},"s":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"∈":{"docs":{},"a":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"π":{"docs":{},"(":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"(":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"​":{"docs":{},"+":{"docs":{},"γ":{"docs":{},"s":{"docs":{},"′":{"docs":{},"∈":{"docs":{},"s":{"docs":{},"∑":{"docs":{},"​":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"′":{"docs":{},"a":{"docs":{},"​":{"docs":{},"v":{"docs":{},"π":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},"′":{"docs":{},")":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"≥":{"docs":{},"v":{"docs":{},"π":{"docs":{},"′":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},",":{"docs":{},"∀":{"docs":{},"s":{"docs":{},"∈":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}},"v":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"v":{"docs":{},"π":{"docs":{},"​":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}},"∗":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"v":{"docs":{},"∗":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"v":{"docs":{},"_":{"docs":{},"{":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"_":{"docs":{},"*":{"docs":{},"}":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}},"∗":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"⁡":{"docs":{},"a":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}},"π":{"docs":{},"v":{"docs":{},"π":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"v":{"docs":{},"∗":{"docs":{},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"v":{"docs":{},"∗":{"docs":{},"​":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"​":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"​":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"i":{"docs":{},"e":{"docs":{},"w":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"|":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0234375},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.011682242990654205}}},"\ud83d":{"docs":{},"\udca1":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}},"\"":{"docs":{},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"l":{"docs":{},"i":{"docs":{},"f":{"docs":{},"e":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{},"\"":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"[":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"k":{"docs":{},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"n":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"e":{"docs":{},"e":{"docs":{},"p":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"π":{"docs":{},"(":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"a":{"docs":{},"|":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}}}}}},":":{"docs":{},"s":{"docs":{},"→":{"docs":{},"a":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},":":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"π":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.021164021164021163},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"∗":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"^":{"docs":{},"*":{"docs":{},"π":{"docs":{},"∗":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"_":{"docs":{},"*":{"docs":{},"π":{"docs":{},"∗":{"docs":{},"​":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"(":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"{":{"1":{"docs":{},"i":{"docs":{},"f":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"docs":{}}}}}}}},"​":{"docs":{},"(":{"docs":{},"a":{"docs":{},"∣":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"⎩":{"docs":{},"⎨":{"docs":{},"⎧":{"docs":{},"​":{"1":{"0":{"docs":{},"​":{"docs":{},"i":{"docs":{},"f":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"docs":{}},"docs":{}}}}}}}}}}}},"≥":{"docs":{},"π":{"docs":{},",":{"docs":{},"∀":{"docs":{},"π":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"_":{"docs":{},"*":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"′":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"'":{"docs":{},"π":{"docs":{},"′":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"≥":{"docs":{},"π":{"docs":{},"′":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"↩":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.016611295681063124}},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"γ":{"docs":{},"\\":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"γ":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"→":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.018691588785046728}}},"\ud83c":{"docs":{},"\udfb2":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"length":788},"corpusTokens":["\"how","\"lifetime\"","&","&=","&\\text{if","&\\text{otherwise}","(","(click","(each","(immediate)","(knowledg","(mdps)","(r_{t+2}","(s),","(s)v∗​(s)=πmax​vπ​(s)","(s,","(γ\\gammaγ",")",")∣st=s]=eπ[rt+1+γgt+1∣st=s]=eπ[rt+1+γvπ(st+1)∣st=s]\\begin{align}","+","0","0,","1","1)","1.","188:","2022","61a).","8","=","=e[r_{t+1}","[","\\\\","\\begin{bmatrix}","\\begin{cases}","\\cdot","\\cdots)","\\cdotss0​a0​​s1​a1​​s2​a2​​⋯","\\displaystyle\\sum_{k=0}^\\infti","\\displaystyle\\sum_{t=0}^\\infti","\\end{align}vπ​(s)​=eπ​[gt​∣st​=s]=eπ​[rt+1​+γrt+2​+γ2rt+3​+⋯∣st​=s]=eπ​[rt+1​+γ(rt+2​+γrt+3​+⋯)∣st​=s]=eπ​[rt+1​+γgt+1​∣st​=s]=eπ​[rt+1​+γvπ​(st+1​)∣st​=s]​​","\\end{bmatrix}top=from⎣⎡​p11​pn1​​⋯⋮⋯​p1n​pnn​​⎦⎤​","\\end{cases}","\\foral","\\frac{r_{max}}{1","\\gamma","\\gamma^2","\\gamma^k","\\gamma^t","\\gamma}gt​=rt+1​+γrt+1​+⋯=k=0∑∞​γkrt+k+1​≤t=0∑∞​γtrmax​=1−γrmax​​","\\geq","\\in","\\leq","\\mathcal{p}","\\mathcal{p}_{s,s'}^a","\\mathcal{p}_{ss'}^a","\\mathcal{r}_s^a","\\mathcal{r}_x^a","\\max_\\pi","\\pi'","\\pi(a'|s')","\\pi(a|s)","\\pi(s)a=π(s)","\\pi,","\\pi_*(a|s)","\\piπ∗​≥π,∀π","\\rightarrow","\\sum_{a","\\sum_{a'","\\sum_{a\\in","\\sum_{s'","\\text{","\\text{from}","\\text{to}","\\underset{a","\\underset{a'}{\\max}","\\underset{a}{\\max}","\\vdot","\\xrightarrow{a_0}","\\xrightarrow{a_1}","\\xrightarrow{a_2}","a')","a')q∗​(s,a)=rsa​+γs′∈s∑​pss′a​a′max​","a)","a)qπ∗​​(s,a)=q∗​(s,a)","a)q∗​(s,a)","a)q∗​(s,a)=πmax​qπ​(s,a)","a)vπ​(s)=a∈a∑​π(a∣s)qπ​(s,a)","a)v∗​(s)=amax​","a,","a=arg⁡max⁡a∈a","a=a∈aargmax​","a=π(s)a","a]ps,s′a​=p[st+1​=s′∣st​=s,at​=a]","a]qπ​(s,a)=eπ​[gt​∣st​=s,at​=a]","a]qπ​(s,a)=eπ​[rt+1​+γqπ​(st+1​,at+1​)∣st​=s,at​=a]","a]rsa​=e[rt+1​∣st​=s,at​=a]","a_t","a_{t+1})","aaa","aa∈a","above,","achiev","action","action,","action.","adapt","affect","after.","agent","agent'","ahead","algorithm","along","alway","and,","animal/human","another.","appli","area","arriv","arrives,","articl","artifici","as:","associ","ata_tat​.","avoid","a}","a}{\\arg\\max}","aπ:s→a,","a∈aa","basic","befor","begin","behavior","behavior.","bellman","berkeley'","best","better","between","both","broad,","but,","call","card","choos","close","closer","combin","compar","compil","compon","condition","consid","construct","conveni","convers","cs","cumul","current","cyclic","david","deal","decid","decis","decompos","deep","defin","degre","depend","describ","determinist","differ","discount","discret","discuss","distribut","do","dozen","e_\\pi","e_\\pi[g_t","e_\\pi[r_{t+1}","each","effect","end","environ","equal","equat","equation.","estim","etc.","evalu","evaluation).","evaluation;","evolv","exercis","exist","expect","experi","express","factor","factors.","far","feedback","field,","find","finit","first","follow","follows:","fulli","function","function,","function.","function:","futur","g_t","g_{t+1}","game","given","goal","gone","good","good\"","goodness/bad","great","greater","greatest","grow","gt=rt+1+rt+1+⋯=∑k=0∞rt+k+1g_t","gt=rt+1+γrt+1+⋯=∑k=0∞γkrt+k+1≤∑t=0∞γtrmax=rmax1−γg_t","gt=∑k=0∞γkrt+k+1g_t","gtg_tgt​","guidebook","guidebook,","guidelin","help","here.","holds:","horizon","however,","hypothesi","i'v","idea","immedi","independ","independent.","indic","infinit","inher","insert","intelligence,","intimid","introduc","introduct","intuitively,","it'","it.","keep","know","known","last","lead","learn","learning?","let'","limit","live","lizard'","longer","look","machin","make","maker","map","markov","mathemat","mathematically,","matrix","matrix.","max","maxim","maximum","mdp","mdp!","mdp,","mdp.","me)","mean","measur","memoryless","memoryless,","model:","more","multipl","multipli","myopic","new","newcomer.","next","nondeterminist","note","notion","now","now,","number","onc","one.","one:","optim","optimality,","order","outlin","output","over","p(s'","p[a_t=a|s_t=s]π(a∣s)=p[at​=a∣st​=s]","p[s_{t+1}","p[st+1∣st]=p[st+1∣s1,⋯st]p[s_{t+1}","p\\mathcal{p}p","p_{11}","p_{1n}","p_{n1}","p_{nn}","pair","particular","past","perform","pictori","pipelin","place","player","poker","polici","policies.","policies:","policy,","policy:","possibl","predict","prefer","present","present.","pretti","previou","prior","probabl","problem","problems,","process","processes,","processes.","program","properti","ps,s′a=p[st+1=s′∣st=s,at=a]","python","q","q_*(s',","q_*(s,","q_\\pi(s',","q_\\pi(s,","q_\\pi(s,a)","q_\\pi(s_{t+1},","qqq","qualiti","question.","qπ(s,a)=eπ[gt∣st=s,at=a]q_\\pi(s,","qπ(s,a)=eπ[rt+1+γqπ(st+1,at+1)∣st=s,at=a]q_\\pi(s,","qπ(s,a)=rsa+γ∑s′∈spss′avπ(s′)","qπ(s,a)=rsa+γ∑s′∈spss′a∑a′∈aπ(a′∣s′)qπ(s′,a′)","qπq_\\piqπ​","qπ​(s,a)=rsa​+γs′∈s∑​pss′a​a′∈a∑​π(a′∣s′)qπ​(s′,a′)","qπ​(s,a)=rsa​+γs′∈s∑​pss′a​vπ​(s′)","qπ∗(s,a)=q∗(s,a)q_{\\pi_*}(s,","q∗(s,a)0otherwis","q∗(s,a)=max⁡πqπ(s,a)q_*(s,a)","q∗(s,a)=rsa+γ∑s′∈spss′amax⁡a′","q∗(s,a)=rxa+γ∑s′∈spss′av∗(s′)","q∗(s,a)q_*(s,","q∗(s,a)v_*(s)","q∗(s′,a′)q_*(s,","q∗q_*q∗​","q∗q_*q∗​,","q∗​(s,a)","q∗​(s,a)=rxa​+γs′∈s∑​pss′a​v∗​(s′)","q∗​(s,a)otherwise​","q∗​(s′,a′)","r_{max}","r_{t+1}","r_{t+2}","r_{t+3}","r_{t+k+1}","r_{t+k+1}gt​=k=0∑∞​γkrt+k+1​","r_{t+k+1}gt​=rt+1​+rt+1​+⋯=k=0∑∞​rt+k+1​","random","rapidli","rate","rate,","realli","recal","recall,","recommend","recurr","recurs","reinforc","relat","relationship","relationship,","relationship:","repres","represent","represented,","resourc","return","return—which","reward","reward.","reward:","rewards,","rewards?","roughli","row","rsa+γ∑s′∈spss′av∗(s′)v_*(s)","rsa=e[rt+1∣st=s,at=a]","rsa​+γs′∈s∑​pss′a​v∗​(s′)","rtr_trt​","s","s'","s')","s,","s0→a0s1→a1s2→a2⋯s_0","s]","s]vπ​(s)=eπ​[gt​∣st​=s]","s_1","s_1,","s_2","s_t","s_t]","s_t]p[st+1​∣st​]=p[st+1​∣s1​,⋯st​]","same","saying,","scalar","scrap","seamless","search","see","seem","set","show","sight","signal","silver'","similarly,","singl","skill","slew","softwar","solv","someon","specif","spring","sss","ss∈","start","state","state,","state.","states.","stochast","structure.","sts_tst​","succeed","successor","sum","s}","sπ≥π′","s′s's′","s∈ss","t(s,a,s′)=p(s′∣s,a).","t(s,a,s′)=p(s′∣s,a)t(s,","t+1t+1t+1","take","take,","take.","termin","terminated.","that'","that,","theorem","thereafter.1","think","those","through","time","time.","timestep","togeth","tool","top=from[p11⋯p1n⋮pn1⋯pnn]","total","transit","ttt","ttt,","ttt:","tutorials,","two","type","uc","unboundedly.","uncertainti","up","us","util","v_*(s')","v_*(s)vπ∗​​(s)=v∗​(s)","v_\\pi","v_\\pi(s')","v_\\pi(s)","v_\\pi(s_{t+1})","v_{\\pi'}","valu","value,","values.","view","vvv","vπ(s)=eπ[gt∣st=s]=eπ[rt+1+γrt+2+γ2rt+3+⋯∣st=s]=eπ[rt+1+γ(rt+2+γrt+3+⋯","vπ(s)=eπ[gt∣st=s]v_\\pi(s)","vπ(s)=∑a∈aπ(a∣s)=(rsa+γ∑s′∈spss′avπ(s′))","vπ(s)=∑a∈aπ(a∣s)qπ(s,a)v_\\pi(s)","vπ(s)≥vπ′(s),∀s∈s\\pi","vπv_\\pivπ​","vπ​(s)=a∈a∑​π(a∣s)=(rsa​+γs′∈s∑​pss′a​vπ​(s′))","vπ​(s)≥vπ′​(s),∀s∈","vπ∗(s)=v∗(s)v_{\\pi_*}(s)","v∗(s)=max⁡a","v∗(s)=max⁡πvπ(s)v_*(s)","v∗(s)v_*(s)v∗​(s)","v∗v_*v∗​,","v∗v_*v∗​:","v∗​(s)=amax​","wait...how","want","way.","we'll","we'r","weight","well","who'","with?","wonder","wondering,","words,","wow!","yield","you.","|","}","}q_*","γ\\gammaγ","γ\\gammaγ,","π(a∣s)=p[at=a∣st=s]\\pi(a|s)","π:s→a\\pi:","π\\piπ","π′\\pi'π′","π∗(a∣s)={1if","π∗\\pi^*π∗","π∗\\pi_*π∗​","π∗​(a∣s)=⎩⎨⎧​10​if","π∗≥π,∀π\\pi_*","π≥π′","→","↩","🎲","💡"],"pipeline":["stopWordFilter","stemmer"]},"store":{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction to Machine Learning\nMachine learning is a broad, rapidly evolving field, and this can be pretty intimidating to a newcomer. As someone who's gone through the process of learning machine learning by scrapping together dozens of tutorials, I want to make this process as seamless as possible for you. I've compiled notes from a slew of resources along with exercises to help you apply the skills you learn along the way.\nTo best maximize your experience with this guidebook, I recommend you have some prior programming experience in Python (knowledge that is roughly comparable to Berkeley's CS 61A). \nHow to use this guidebook\nInsert some guidelines here.\n"},"the-basics/overview.html":{"url":"the-basics/overview.html","title":"The Basics","keywords":"","body":""},"the-basics/what-is-ml.html":{"url":"the-basics/what-is-ml.html","title":"What is machine learning?","keywords":"","body":"What is machine learning?\n"},"the-basics/types-of-ml.html":{"url":"the-basics/types-of-ml.html","title":"Types of machine learning","keywords":"","body":"Types of machine learning\n"},"the-basics/ml-pipeline.html":{"url":"the-basics/ml-pipeline.html","title":"Machine learning pipeline","keywords":"","body":"The machine learning pipeline\n"},"the-basics/software-tools.html":{"url":"the-basics/software-tools.html","title":"Software tools","keywords":"","body":"Software tools\n"},"reinforcement-learning/overview.html":{"url":"reinforcement-learning/overview.html","title":"Reinforcement Learning","keywords":"","body":"Reinforcement learning\nReinforcement learning is an area of machine learning that deals with how agents take actions on their environments in order to maximize some reward.\n"},"reinforcement-learning/bellman-eqn.html":{"url":"reinforcement-learning/bellman-eqn.html","title":"Bellman Equation","keywords":"","body":"Bellman Equation\nLet's take a closer look at the value function that we discussed last time. We will decompose it so that we can see a recursive structure.\nvπ(s)=Eπ[Gt∣St=s]=Eπ[Rt+1+γRt+2+γ2Rt+3+⋯∣St=s]=Eπ[Rt+1+γ(Rt+2+γRt+3+⋯ )∣St=s]=Eπ[Rt+1+γGt+1∣St=s]=Eπ[Rt+1+γvπ(St+1)∣St=s]\\begin{align}\nv_\\pi(s) &= E_\\pi[G_t | S_t = s] \\\\ \n&= E_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s] \\\\\n&= E_\\pi[R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) | S_t = s] \\\\\n&= E_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n&= E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\\\n\\end{align}vπ​(s)​=Eπ​[Gt​∣St​=s]=Eπ​[Rt+1​+γRt+2​+γ2Rt+3​+⋯∣St​=s]=Eπ​[Rt+1​+γ(Rt+2​+γRt+3​+⋯)∣St​=s]=Eπ​[Rt+1​+γGt+1​∣St​=s]=Eπ​[Rt+1​+γvπ​(St+1​)∣St​=s]​​\nSimilarly, we can do the same for the Q-function:\nqπ(s,a)=Eπ[Rt+1+γqπ(St+1,At+1)∣St=s,At=a]q_\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]qπ​(s,a)=Eπ​[Rt+1​+γqπ​(St+1​,At+1​)∣St​=s,At​=a]\nNow, when we consider the value function, what it is really doing is it's looking ahead from its current state and performing a weighted sum of all the all the possible actions from that state to the next one. For each of those actions that we can take, there is a Q-value associated with it. Mathematically, we can express this as: \nvπ(s)=∑a∈Aπ(a∣s)qπ(s,a)v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) q_\\pi(s, a)vπ​(s)=a∈A∑​π(a∣s)qπ​(s,a)\n\n\n  💡 Pictorial representation of the relationship between qqq and vvv (click me)  \n\n \n\n\nNow, to find the converse relationship, we now consider that we're in a given state, and we are wondering how good it is to take a specific action. For this action, we will get an immediate reward and will then end up in a specific state as given by the state transition matrix. We now can use the value function to find the expected return from this new state. Mathematically, we can express this as:\nqπ(s,a)=Rsa+γ∑s′∈SPss′avπ(s′) q_\\pi(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a v_\\pi(s') qπ​(s,a)=Rsa​+γs′∈S∑​Pss′a​vπ​(s′)\nCombining the two representations above, we find a recurrence relation for the value function and Q-function as follows:\nvπ(s)=∑a∈Aπ(a∣s)=(Rsa+γ∑s′∈SPss′avπ(s′)) v_\\pi(s) = \\sum_{a\\in A} \\pi(a|s) = ( \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a v_\\pi(s') ) vπ​(s)=a∈A∑​π(a∣s)=(Rsa​+γs′∈S∑​Pss′a​vπ​(s′))\nqπ(s,a)=Rsa+γ∑s′∈SPss′a∑a′∈Aπ(a′∣s′)qπ(s′,a′) q_\\pi(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a \\sum_{a' \\in A} \\pi(a'|s') q_\\pi(s', a') qπ​(s,a)=Rsa​+γs′∈S∑​Pss′a​a′∈A∑​π(a′∣s′)qπ​(s′,a′)\n\nAdapted from UC Berkeley's CS 188: Introduction to Artificial Intelligence, Spring 2022 Note 8 and David Silver's Markov Decision Processes\n"},"reinforcement-learning/policies-values.html":{"url":"reinforcement-learning/policies-values.html","title":"Policies and Value Functions","keywords":"","body":"Policies and Value Functions\nNow, we discuss how do we measure \"how good\" it is for an agent to choose a given action in a specific state. A policy is the agent's behavior. A value function is a prediction of the future reward that is used to evaluate the goodness/badness of states.\nPolicies\nA policy, π:S→A\\pi: S \\rightarrow Aπ:S→A, is a function that maps each state to an action. \n\nA deterministic policy: a=π(s)a = \\pi(s)a=π(s) describes how to get from some state sss to some action aaa\nA stochastic policy: π(a∣s)=P[At=a∣St=s]\\pi(a|s) = P[A_t=a|S_t=s]π(a∣s)=P[At​=a∣St​=s] is a distribution over actions given states\n\nThe optimal policy is a policy π∗\\pi^*π∗ that has an expected return that is greater than or equal to the expected return or utility for all states as compared to all policies.\nValue Functions\nValue functions are functions of states or of state-action pairs that estimate how good it is for an agent to be in a specific state or how good a specific action is in a given state.\nThe state-value function for policy π\\piπ is vπv_\\pivπ​ is the expected return an agent starting at state sss at time ttt that follows policy π\\piπ thereafter.1\nvπ(s)=Eπ[Gt∣St=s]v_\\pi(s) = E_\\pi[G_t | S_t = s]vπ​(s)=Eπ​[Gt​∣St​=s]\nThe action-value function for policy π\\piπ is qπq_\\piqπ​ is the expected return from starting at state sss at time ttt taking action aaa that follows policy π\\piπ after. \nqπ(s,a)=Eπ[Gt∣St=s,At=a]q_\\pi(s, a) = E_\\pi [ G_t | S_t = s, A_t = a]qπ​(s,a)=Eπ​[Gt​∣St​=s,At​=a]\nThis is also known as the Q-function, and the output from the function is called a Q-value, where Q represents the quality of a given action in a given state.\n\n1. Recall, Gt=∑k=0∞γkRt+k+1G_t = \\displaystyle\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}Gt​=k=0∑∞​γkRt+k+1​ ↩\n\n\nAdapted from UC Berkeley's CS 188: Introduction to Artificial Intelligence, Spring 2022 Note 8 and Deep Lizard's Policies and Value Functions Article\n"},"reinforcement-learning/optimality.html":{"url":"reinforcement-learning/optimality.html","title":"Optimal Policies and Value Functions","keywords":"","body":"Optimality\nOptimal Value Function\nThe optimal value function outlines the best possible performance in an MDP, and this is why an MDP is effectively solved when we know the optimal value function. \nThe optimal state-value function v∗(s)v_*(s)v∗​(s) is the maximum value function over all policies: \nv∗(s)=max⁡πvπ(s)v_*(s) = \\max_\\pi v_\\pi (s)v∗​(s)=πmax​vπ​(s)\nThe optimal action-value function q∗(s,a)q_*(s, a)q∗​(s,a) is the maximum action-value function over all policies: \nq∗(s,a)=max⁡πqπ(s,a)q_*(s,a) = \\max_\\pi q_\\pi(s, a)q∗​(s,a)=πmax​qπ​(s,a)\nOptimal Policy\nBefore we consider optimality, we will first introduce this notion of what makes a policy better than another. A policy π\\piπ is better than a policy π′\\pi'π′ if the following holds: \nπ≥π′ if vπ(s)≥vπ′(s),∀s∈S\\pi \\geq \\pi' \\text{ if } v_\\pi(s) \\geq v_{\\pi'} (s), \\forall s \\in Sπ≥π′ if vπ​(s)≥vπ′​(s),∀s∈S\n\n💡 Theorem\nFor any MDP, \n\n There exists an optimal policy π∗\\pi_*π∗​ that is better than or equal to all other policies: π∗≥π,∀π\\pi_* \\geq \\pi, \\forall \\piπ∗​≥π,∀π \n All optimal policies achieve the optimal value function, vπ∗(s)=v∗(s)v_{\\pi_*}(s) = v_*(s)vπ∗​​(s)=v∗​(s)\n All optimal policies achieve the optimal action-value function, qπ∗(s,a)=q∗(s,a)q_{\\pi_*}(s, a) = q_*(s, a)qπ∗​​(s,a)=q∗​(s,a)\n\n\nFinding an Optimal Policy\nRecall that there is always a deterministic optimal policy for any MDP. We will now describe an algorithm to find one:\nπ∗(a∣s)={1if a=arg⁡max⁡a∈A q∗(s,a)0otherwise\n\\pi_*(a|s) = \\begin{cases}\n   1 &\\text{if } a = \\underset{a \\in A}{\\arg\\max} \\text{ }q_* (s, a) \\\\\n   0 &\\text{otherwise} \n\\end{cases}\nπ∗​(a∣s)=⎩⎨⎧​10​if a=a∈Aargmax​ q∗​(s,a)otherwise​\nYou might be wondering, wow! once we have q∗q_*q∗​, we can solve any MDP! But, wait...how do we get q∗q_*q∗​ to begin with? And, that's a great question. For that, we'll introduce the Bellman Optimality Equation.\nBellman Optimality Equation\nFor v∗v_*v∗​, we define the following relationship:\nv∗(s)=max⁡a q∗(s,a)v_*(s) = \\underset{a}{\\max} \\text{ } q_*(s, a)v∗​(s)=amax​ q∗​(s,a)\nIntuitively, this function is saying, at a particular state, we look ahead to all the possible actions we can possibly take, and we choose the action that yields the greatest expected return—which is just the max of all the Q-values. \nFor q∗q_*q∗​, we define the following relationship:\nq∗(s,a)=Rxa+γ∑s′∈SPss′av∗(s′) q_*(s, a) = \\mathcal{R}_x^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a v_*(s') q∗​(s,a)=Rxa​+γs′∈S∑​Pss′a​v∗​(s′)\nIntuitively, we have the immediate reward from taking the given action, and then we sum over the probabilities of starting in the current state and ending up in a different state multiplied by the value function of each ending state.\nWe can now introduce recursive relationships for both q∗q_*q∗​ and v∗v_*v∗​:\nv∗(s)=max⁡a Rsa+γ∑s′∈SPss′av∗(s′)v_*(s) = \\underset{a}{\\max} \\text{ } \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a v_*(s') v∗​(s)=amax​ Rsa​+γs′∈S∑​Pss′a​v∗​(s′)\nq∗(s,a)=Rsa+γ∑s′∈SPss′amax⁡a′ q∗(s′,a′)q_*(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a \\underset{a'}{\\max} \\text{ } q_*(s', a')q∗​(s,a)=Rsa​+γs′∈S∑​Pss′a​a′max​ q∗​(s′,a′)\n\nAdapted from David Silver's Markov Decision Processes\n"},"reinforcement-learning/mdp.html":{"url":"reinforcement-learning/mdp.html","title":"Markov Decision Processes (MDPs)","keywords":"","body":"Markov Decision Processes (MDPs)\nAn agent that is placed in an environment where there are multiple possible successor states from a single action in some state is a nondeterministic action. Think about a card game like poker where the randomness of dealing cards can introduce uncertainty into the succeeding actions a player might take. Problems that have an inherent degree of uncertainty are nondeterministic search problems, and they can be solved with Markov Decision Processes.\nComponents of an MDP\n\nAgent → the decision maker\nEnvironment → where the agent lives\nSet of States SSS → the representation of the environment at a given time\nA start state\n1 or more terminal states → a state where once the agent arrives, it can no longer perform any actions for more rewards\n\n\nSet of Actions AAA → what the agent decides to do at a state\nReward RtR_tRt​ → a scalar feedback signal that indicates how well an agent is doing at timestep ttt\nState Transition Probability → a probability function that represents the probability that an agent takes an action a∈Aa \\in Aa∈A from a state s∈Ss \\in Ss∈S and ends up in state s′s's′\nPs,s′a=P[St+1=s′∣St=s,At=a] \\mathcal{P}_{s,s'}^a = P[S_{t+1} = s' | S_t = s, A_t = a]Ps,s′a​=P[St+1​=s′∣St​=s,At​=a]\nState transition matrix P\\mathcal{P}P defines transition probabilities from all states sss to all successor states s′s's′ (each row sums to 1)\ntoP=from[P11⋯P1n⋮Pn1⋯Pnn]\n    \\text{to} \\\\\n    \\mathcal{P} =     \\text{from} \n    \\begin{bmatrix}\n    \n    P_{11} & \\cdots & P_{1n} \\\\\n    & \\vdots & \\\\\n    P_{n1} & \\cdots & P_{nn} \\\\\n    \\end{bmatrix}toP=from⎣⎡​P11​Pn1​​⋯⋮⋯​P1n​Pnn​​⎦⎤​\n\n\nReward Function → predicts the next (immediate) reward\nRsa=E[Rt+1∣St=s,At=a] \\mathcal{R}_s^a =E[R_{t+1} | S_t = s, A_t = a]Rsa​=E[Rt+1​∣St​=s,At​=a]\n\n\n💡 Reward Hypothesis\nAll goals can be described by the maximization of expected cumulative reward \n\nReturn and Discounting\nWe can view the agent's states through discrete timesteps where at timestep ttt, an agent is at state sts_tst​ and takes action ata_tat​. Starting at timestep 0, we can construct the following model:\ns0→a0s1→a1s2→a2⋯s_0 \\xrightarrow{a_0} s_1 \\xrightarrow{a_1} s_2 \\xrightarrow{a_2} \\cdotss0​a0​​s1​a1​​s2​a2​​⋯\nThe return GtG_tGt​ is the total reward from timestep ttt:\nGt=Rt+1+Rt+1+⋯=∑k=0∞Rt+k+1G_t = R_{t+1} + R_{t+1} + \\cdots = \\displaystyle\\sum_{k=0}^\\infty R_{t+k+1}Gt​=Rt+1​+Rt+1​+⋯=k=0∑∞​Rt+k+1​\nIt may seem like if you keep choosing the same optimal state-action-reward set that you can get your utility function to equal a value that grows unboundedly. However, we now introduce this idea of finite horizons and discount factors. A finite horizon is a \"lifetime\" for agents that limits the number of timesteps to maximize reward before the agent is terminated. A discount factor or discount rate, γ\\gammaγ, is a number between 0 and 1 that is the rate for which we discount future rewards (γ\\gammaγ close to 0 leads to myopic evaluation; γ\\gammaγ close to 1 leads to far-sighted evaluation). \nNow, we can say that our agent's goal is to maximize the total discounted reward:\nGt=Rt+1+γRt+1+⋯=∑k=0∞γkRt+k+1≤∑t=0∞γtRmax=Rmax1−γG_t = R_{t+1} + \\gamma R_{t+1} + \\cdots = \\displaystyle\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\leq \\displaystyle\\sum_{t=0}^\\infty \\gamma^t R_{max} = \\frac{R_{max}}{1-\\gamma}Gt​=Rt+1​+γRt+1​+⋯=k=0∑∞​γkRt+k+1​≤t=0∑∞​γtRmax​=1−γRmax​​\n\n\n  🎲 Why do we use discounted rewards? (click me)  \nMathematically convenient to discount rewards, avoids infinite returns in cyclic Markov processes, uncertainty about the future may not be fully represented, animal/human behaviors show preference for immediate rewards, etc.\n\n\n\n\n\nMemoryless Property\n MDPs are memoryless, meaning that only the current state affects the future state, and the future and the past are conditionally independent. Mathematically, we can express this as: T(s,a,s′)=P(s′∣s,a)T(s, a, s') = P(s' | s, a) T(s,a,s′)=P(s′∣s,a). In other words, the probability of arriving to state s′s's′ at time t+1t+1t+1 only depends on the current state at time ttt and not any of the previous states.\n The future is independent of the past given the present.\n\n💡 Markov or Memoryless Property\nThe future is independent of the past given the present  \nP[St+1∣St]=P[St+1∣S1,⋯St]P[S_{t+1} | S_t] = P[S_{t+1} | S_1, \\cdots S_t]P[St+1​∣St​]=P[St+1​∣S1​,⋯St​] \n\n\nAdapted from UC Berkeley's CS 188: Introduction to Artificial Intelligence, Spring 2022 Note 8\n"}}}
{"index":{"version":"0.5.12","fields":[{"name":"title","boost":10},{"name":"keywords","boost":15},{"name":"body","boost":1}],"ref":"url","documentStore":{"store":{"./":["(knowledg","61a).","along","appli","berkeley'","best","broad,","compar","compil","cs","dozen","evolv","exercis","experi","field,","gone","guidebook","guidebook,","guidelin","help","here.","i'v","insert","intimid","introduct","learn","machin","make","maxim","newcomer.","note","possibl","pretti","prior","process","program","python","rapidli","recommend","resourc","roughli","scrap","seamless","skill","slew","someon","through","togeth","tutorials,","us","want","way.","who'","you."],"the-basics/overview.html":["basic"],"the-basics/what-is-ml.html":["learning?","machin"],"the-basics/types-of-ml.html":["learn","machin","type"],"the-basics/ml-pipeline.html":["learn","machin","pipelin"],"the-basics/software-tools.html":["softwar","tool"],"reinforcement-learning/overview.html":["action","agent","area","deal","environ","learn","machin","maxim","order","reinforc","reward.","take"],"reinforcement-learning/bellman-eqn.html":["&=","(","(click","(r_{t+2}",")",")‚à£st=s]=eœÄ[rt+1+Œ≥gt+1‚à£st=s]=eœÄ[rt+1+Œ≥vœÄ(st+1)‚à£st=s]\\begin{align}","+","188:","2022","8","=","\\\\","\\cdot","\\cdots)","\\end{align}vœÄ‚Äã(s)‚Äã=eœÄ‚Äã[gt‚Äã‚à£st‚Äã=s]=eœÄ‚Äã[rt+1‚Äã+Œ≥rt+2‚Äã+Œ≥2rt+3‚Äã+‚ãØ‚à£st‚Äã=s]=eœÄ‚Äã[rt+1‚Äã+Œ≥(rt+2‚Äã+Œ≥rt+3‚Äã+‚ãØ)‚à£st‚Äã=s]=eœÄ‚Äã[rt+1‚Äã+Œ≥gt+1‚Äã‚à£st‚Äã=s]=eœÄ‚Äã[rt+1‚Äã+Œ≥vœÄ‚Äã(st+1‚Äã)‚à£st‚Äã=s]‚Äã‚Äã","\\gamma","\\gamma^2","\\in","\\mathcal{p}_{ss'}^a","\\mathcal{r}_s^a","\\pi(a'|s')","\\pi(a|s)","\\sum_{a","\\sum_{a'","\\sum_{a\\in","\\sum_{s'","a')","a)","a)vœÄ‚Äã(s)=a‚ààa‚àë‚ÄãœÄ(a‚à£s)qœÄ‚Äã(s,a)","a]qœÄ‚Äã(s,a)=eœÄ‚Äã[rt+1‚Äã+Œ≥qœÄ‚Äã(st+1‚Äã,at+1‚Äã)‚à£st‚Äã=s,at‚Äã=a]","a_t","a_{t+1})","above,","action","action,","action.","adapt","ahead","artifici","as:","associ","a}","bellman","berkeley'","between","closer","combin","consid","convers","cs","current","david","decis","decompos","discuss","do","e_\\pi[g_t","e_\\pi[r_{t+1}","each","end","equat","expect","express","find","follows:","function","function,","function:","g_{t+1}","given","good","immedi","intelligence,","introduct","it'","it.","last","let'","look","markov","mathematically,","matrix.","me)","new","next","note","now","now,","one.","perform","pictori","possibl","process","q","q_\\pi(s',","q_\\pi(s,","q_\\pi(s,a)","q_\\pi(s_{t+1},","qqq","qœÄ(s,a)=eœÄ[rt+1+Œ≥qœÄ(st+1,at+1)‚à£st=s,at=a]q_\\pi(s,","qœÄ(s,a)=rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤avœÄ(s‚Ä≤)","qœÄ(s,a)=rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤a‚àëa‚Ä≤‚ààaœÄ(a‚Ä≤‚à£s‚Ä≤)qœÄ(s‚Ä≤,a‚Ä≤)","qœÄ‚Äã(s,a)=rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚Äãa‚Ä≤‚ààa‚àë‚ÄãœÄ(a‚Ä≤‚à£s‚Ä≤)qœÄ‚Äã(s‚Ä≤,a‚Ä≤)","qœÄ‚Äã(s,a)=rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚ÄãvœÄ‚Äã(s‚Ä≤)","r_{t+2}","r_{t+3}","realli","recurr","recurs","relat","relationship","relationship,","represent","return","reward","s,","s]","s_t","same","see","silver'","similarly,","specif","spring","state","state,","state.","structure.","sum","s}","take","take,","those","time.","transit","two","uc","up","us","v_\\pi(s')","v_\\pi(s)","v_\\pi(s_{t+1})","valu","vvv","vœÄ(s)=eœÄ[gt‚à£st=s]=eœÄ[rt+1+Œ≥rt+2+Œ≥2rt+3+‚ãØ‚à£st=s]=eœÄ[rt+1+Œ≥(rt+2+Œ≥rt+3+‚ãØ","vœÄ(s)=‚àëa‚ààaœÄ(a‚à£s)=(rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤avœÄ(s‚Ä≤))","vœÄ(s)=‚àëa‚ààaœÄ(a‚à£s)qœÄ(s,a)v_\\pi(s)","vœÄ‚Äã(s)=a‚ààa‚àë‚ÄãœÄ(a‚à£s)=(rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚ÄãvœÄ‚Äã(s‚Ä≤))","we'r","weight","wonder","|","üí°"],"reinforcement-learning/policies-values.html":["\"how","1.","188:","2022","8","=","[","\\displaystyle\\sum_{k=0}^\\infti","\\gamma^k","\\pi(s)a=œÄ(s)","\\rightarrow","a)","a=œÄ(s)a","a]qœÄ‚Äã(s,a)=eœÄ‚Äã[gt‚Äã‚à£st‚Äã=s,at‚Äã=a]","a_t","aaa","action","action.","adapt","after.","agent","agent'","articl","artifici","aœÄ:s‚Üía,","behavior.","berkeley'","call","choos","compar","cs","deep","describ","determinist","discuss","distribut","e_\\pi","e_\\pi[g_t","each","equal","estim","evalu","expect","follow","function","function,","futur","g_t","given","good","good\"","goodness/bad","greater","gt=‚àëk=0‚àûŒ≥krt+k+1g_t","intelligence,","introduct","known","lizard'","map","measur","note","now,","optim","output","over","p[a_t=a|s_t=s]œÄ(a‚à£s)=p[at‚Äã=a‚à£st‚Äã=s]","pair","polici","policies.","policy,","policy:","predict","q","qualiti","qœÄ(s,a)=eœÄ[gt‚à£st=s,at=a]q_\\pi(s,","qœÄq_\\piqœÄ‚Äã","r_{t+k+1}gt‚Äã=k=0‚àë‚àû‚ÄãŒ≥krt+k+1‚Äã","recall,","repres","return","reward","s","s,","s]vœÄ‚Äã(s)=eœÄ‚Äã[gt‚Äã‚à£st‚Äã=s]","s_t","specif","spring","sss","start","state","state.","states.","stochast","take","thereafter.1","time","ttt","uc","us","util","valu","value,","vœÄ(s)=eœÄ[gt‚à£st=s]v_\\pi(s)","vœÄv_\\pivœÄ‚Äã","|","œÄ(a‚à£s)=p[at=a‚à£st=s]\\pi(a|s)","œÄ:s‚Üía\\pi:","œÄ\\piœÄ","œÄ‚àó\\pi^*œÄ‚àó","‚Ü©"],"reinforcement-learning/optimality.html":["&\\text{if","&\\text{otherwise}","(s),","(s)v‚àó‚Äã(s)=œÄmax‚ÄãvœÄ‚Äã(s)","(s,","+","0","1","=","\\\\","\\begin{cases}","\\end{cases}","\\foral","\\gamma","\\geq","\\in","\\mathcal{p}_{ss'}^a","\\mathcal{r}_s^a","\\mathcal{r}_x^a","\\max_\\pi","\\pi'","\\pi,","\\pi_*(a|s)","\\piœÄ‚àó‚Äã‚â•œÄ,‚àÄœÄ","\\sum_{s'","\\text{","\\underset{a","\\underset{a'}{\\max}","\\underset{a}{\\max}","a')q‚àó‚Äã(s,a)=rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚Äãa‚Ä≤max‚Äã","a)","a)qœÄ‚àó‚Äã‚Äã(s,a)=q‚àó‚Äã(s,a)","a)q‚àó‚Äã(s,a)","a)q‚àó‚Äã(s,a)=œÄmax‚ÄãqœÄ‚Äã(s,a)","a)v‚àó‚Äã(s)=amax‚Äã","a=arg‚Å°max‚Å°a‚ààa","a=a‚ààaargmax‚Äã","achiev","action","action,","adapt","ahead","algorithm","alway","and,","another.","a}{\\arg\\max}","befor","begin","bellman","best","better","both","but,","choos","consid","current","david","decis","defin","describ","determinist","differ","each","effect","end","equal","equat","equation.","exist","expect","find","first","follow","function","function,","function.","given","great","greatest","holds:","immedi","introduc","intuitively,","know","look","make","markov","max","maximum","mdp","mdp!","mdp,","mdp.","multipli","notion","now","onc","one:","optim","optimality,","outlin","over","particular","perform","polici","policies:","possibl","probabl","process","q","q_*(s',","q_*(s,","q_\\pi(s,","question.","qœÄ‚àó(s,a)=q‚àó(s,a)q_{\\pi_*}(s,","q‚àó(s,a)0otherwis","q‚àó(s,a)=max‚Å°œÄqœÄ(s,a)q_*(s,a)","q‚àó(s,a)=rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤amax‚Å°a‚Ä≤","q‚àó(s,a)=rxa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤av‚àó(s‚Ä≤)","q‚àó(s,a)q_*(s,","q‚àó(s,a)v_*(s)","q‚àó(s‚Ä≤,a‚Ä≤)q_*(s,","q‚àóq_*q‚àó‚Äã","q‚àóq_*q‚àó‚Äã,","q‚àó‚Äã(s,a)","q‚àó‚Äã(s,a)=rxa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚Äãv‚àó‚Äã(s‚Ä≤)","q‚àó‚Äã(s,a)otherwise‚Äã","q‚àó‚Äã(s‚Ä≤,a‚Ä≤)","recal","recurs","relationship","relationship:","return‚Äîwhich","reward","rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤av‚àó(s‚Ä≤)v_*(s)","rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚Äãv‚àó‚Äã(s‚Ä≤)","s","saying,","silver'","solv","start","state","state,","state.","sum","s}","sœÄ‚â•œÄ‚Ä≤","take","take,","that'","that,","theorem","up","v_*(s')","v_*(s)vœÄ‚àó‚Äã‚Äã(s)=v‚àó‚Äã(s)","v_\\pi","v_\\pi(s)","v_{\\pi'}","valu","values.","vœÄ(s)‚â•vœÄ‚Ä≤(s),‚àÄs‚ààs\\pi","vœÄ‚Äã(s)‚â•vœÄ‚Ä≤‚Äã(s),‚àÄs‚àà","vœÄ‚àó(s)=v‚àó(s)v_{\\pi_*}(s)","v‚àó(s)=max‚Å°a","v‚àó(s)=max‚Å°œÄvœÄ(s)v_*(s)","v‚àó(s)v_*(s)v‚àó‚Äã(s)","v‚àóv_*v‚àó‚Äã,","v‚àóv_*v‚àó‚Äã:","v‚àó‚Äã(s)=amax‚Äã","wait...how","we'll","with?","wondering,","wow!","yield","}","}q_*","œÄ\\piœÄ","œÄ‚Ä≤\\pi'œÄ‚Ä≤","œÄ‚àó(a‚à£s)={1if","œÄ‚àó\\pi_*œÄ‚àó‚Äã","œÄ‚àó‚Äã(a‚à£s)=‚é©‚é®‚éß‚Äã10‚Äãif","œÄ‚àó‚â•œÄ,‚àÄœÄ\\pi_*","œÄ‚â•œÄ‚Ä≤","üí°"],"reinforcement-learning/mdp.html":["\"lifetime\"","&","(click","(each","(immediate)","(mdps)","(Œ≥\\gammaŒ≥","+","0","0,","1","1)","188:","2022","8","=","=e[r_{t+1}","\\\\","\\begin{bmatrix}","\\cdot","\\cdotss0‚Äãa0‚Äã‚Äãs1‚Äãa1‚Äã‚Äãs2‚Äãa2‚Äã‚Äã‚ãØ","\\displaystyle\\sum_{k=0}^\\infti","\\displaystyle\\sum_{t=0}^\\infti","\\end{bmatrix}top=from‚é£‚é°‚Äãp11‚Äãpn1‚Äã‚Äã‚ãØ‚ãÆ‚ãØ‚Äãp1n‚Äãpnn‚Äã‚Äã‚é¶‚é§‚Äã","\\frac{r_{max}}{1","\\gamma","\\gamma^k","\\gamma^t","\\gamma}gt‚Äã=rt+1‚Äã+Œ≥rt+1‚Äã+‚ãØ=k=0‚àë‚àû‚ÄãŒ≥krt+k+1‚Äã‚â§t=0‚àë‚àû‚ÄãŒ≥trmax‚Äã=1‚àíŒ≥rmax‚Äã‚Äã","\\in","\\leq","\\mathcal{p}","\\mathcal{p}_{s,s'}^a","\\mathcal{r}_s^a","\\text{from}","\\text{to}","\\vdot","\\xrightarrow{a_0}","\\xrightarrow{a_1}","\\xrightarrow{a_2}","a)","a,","a]ps,s‚Ä≤a‚Äã=p[st+1‚Äã=s‚Ä≤‚à£st‚Äã=s,at‚Äã=a]","a]rsa‚Äã=e[rt+1‚Äã‚à£st‚Äã=s,at‚Äã=a]","a_t","aaa","aa‚ààa","action","action.","adapt","affect","agent","agent'","animal/human","arriv","arrives,","artifici","as:","ata_tat‚Äã.","avoid","a‚ààaa","befor","behavior","berkeley'","between","card","choos","close","compon","condition","construct","conveni","cs","cumul","current","cyclic","deal","decid","decis","defin","degre","depend","describ","discount","discret","do","end","environ","equal","etc.","evaluation).","evaluation;","expect","express","factor","factors.","far","feedback","finit","follow","fulli","function","futur","game","given","goal","grow","gt=rt+1+rt+1+‚ãØ=‚àëk=0‚àûrt+k+1g_t","gt=rt+1+Œ≥rt+1+‚ãØ=‚àëk=0‚àûŒ≥krt+k+1‚â§‚àët=0‚àûŒ≥trmax=rmax1‚àíŒ≥g_t","gtg_tgt‚Äã","horizon","however,","hypothesi","idea","immedi","independ","independent.","indic","infinit","inher","intelligence,","introduc","introduct","keep","lead","limit","live","longer","maker","markov","mathemat","mathematically,","matrix","maxim","mdp","me)","mean","memoryless","memoryless,","model:","more","multipl","myopic","next","nondeterminist","note","now","now,","number","onc","optim","p(s'","p[s_{t+1}","p[st+1‚à£st]=p[st+1‚à£s1,‚ãØst]p[s_{t+1}","p\\mathcal{p}p","p_{11}","p_{1n}","p_{n1}","p_{nn}","past","perform","place","player","poker","possibl","predict","prefer","present","present.","previou","probabl","problem","problems,","process","processes,","processes.","properti","ps,s‚Ä≤a=p[st+1=s‚Ä≤‚à£st=s,at=a]","r_{max}","r_{t+1}","r_{t+k+1}","r_{t+k+1}gt‚Äã=rt+1‚Äã+rt+1‚Äã+‚ãØ=k=0‚àë‚àû‚Äãrt+k+1‚Äã","random","rate","rate,","repres","represent","represented,","return","reward","reward:","rewards,","rewards?","row","rsa=e[rt+1‚à£st=s,at=a]","rtr_trt‚Äã","s'","s')","s,","s0‚Üía0s1‚Üía1s2‚Üía2‚ãØs_0","s_1","s_1,","s_2","s_t","s_t]","s_t]p[st+1‚Äã‚à£st‚Äã]=p[st+1‚Äã‚à£s1‚Äã,‚ãØst‚Äã]","same","scalar","search","seem","set","show","sight","signal","singl","solv","spring","sss","ss‚àà","start","state","state,","states.","sts_tst‚Äã","succeed","successor","sum","s‚Ä≤s's‚Ä≤","s‚ààss","t(s,a,s‚Ä≤)=p(s‚Ä≤‚à£s,a).","t(s,a,s‚Ä≤)=p(s‚Ä≤‚à£s,a)t(s,","t+1t+1t+1","take","take.","termin","terminated.","think","through","time","timestep","top=from[p11‚ãØp1n‚ãÆpn1‚ãØpnn]","total","transit","ttt","ttt,","ttt:","uc","unboundedly.","uncertainti","up","us","util","valu","view","well","words,","|","Œ≥\\gammaŒ≥","Œ≥\\gammaŒ≥,","‚Üí","üé≤","üí°"]},"length":11},"tokenStore":{"root":{"0":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"1":{"8":{"8":{"docs":{},":":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}},"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},")":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"2":{"0":{"2":{"2":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{}},"docs":{}},"docs":{}},"6":{"1":{"docs":{},"a":{"docs":{},")":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"docs":{}},"8":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{},"(":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"k":{"docs":{},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{},"l":{"docs":{},"e":{"docs":{},"d":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}}}},"c":{"docs":{},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{},"k":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"r":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"docs":{}}}}}},"s":{"docs":{},")":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},"v":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"œÄ":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Äã":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"e":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"i":{"docs":{},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},")":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"m":{"docs":{},"d":{"docs":{},"p":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":2.5023364485981308}}}}}}},"Œ≥":{"docs":{},"\\":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"Œ≥":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.031746031746031744}}}}},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"w":{"docs":{},"a":{"docs":{},"y":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"p":{"docs":{},"p":{"docs":{},"l":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.042328042328042326},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.016611295681063124},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.016355140186915886}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"h":{"docs":{},"i":{"docs":{},"e":{"docs":{},"v":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.02336448598130841}},"'":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667}}}},"t":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"c":{"docs":{},"l":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"r":{"docs":{},"i":{"docs":{},"v":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"'":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"q":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"‚àë":{"docs":{},"‚Äã":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.013289036544850499},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"v":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"‚àà":{"docs":{},"a":{"docs":{},"‚àë":{"docs":{},"‚Äã":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},")":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}},"q":{"docs":{},"œÄ":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"q":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"=":{"docs":{},"œÄ":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Äã":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}},"]":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},")":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}},"docs":{}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}},"g":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"p":{"docs":{},"s":{"docs":{},",":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"e":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"docs":{}}}}},"b":{"docs":{},"o":{"docs":{},"v":{"docs":{},"e":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"d":{"docs":{},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"h":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"s":{"docs":{},":":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875}},"{":{"docs":{},"\\":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"\\":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"=":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"a":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"‚Å°":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Å°":{"docs":{},"a":{"docs":{},"‚àà":{"docs":{},"a":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}},"‚àà":{"docs":{},"a":{"docs":{},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"a":{"docs":{},"a":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"‚àà":{"docs":{},"a":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"f":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"œÄ":{"docs":{},":":{"docs":{},"s":{"docs":{},"‚Üí":{"docs":{},"a":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}},"n":{"docs":{},"d":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{},"/":{"docs":{},"h":{"docs":{},"u":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"t":{"docs":{},"a":{"docs":{},"_":{"docs":{},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"v":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"‚àà":{"docs":{},"a":{"docs":{},"a":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{},"k":{"docs":{},"e":{"docs":{},"l":{"docs":{},"e":{"docs":{},"y":{"docs":{},"'":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"l":{"docs":{},"l":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":5.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}},"t":{"docs":{},"w":{"docs":{},"e":{"docs":{},"e":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}},"h":{"docs":{},"a":{"docs":{},"v":{"docs":{},"i":{"docs":{},"o":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"r":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"a":{"docs":{},"s":{"docs":{},"i":{"docs":{},"c":{"docs":{"the-basics/overview.html":{"ref":"the-basics/overview.html","tf":10}}}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"u":{"docs":{},"t":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"i":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"b":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"n":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"d":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"s":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"l":{"docs":{},"o":{"docs":{},"s":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"r":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}},"m":{"docs":{},"u":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"r":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"h":{"docs":{},"o":{"docs":{},"o":{"docs":{},"s":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"y":{"docs":{},"c":{"docs":{},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"d":{"docs":{},"o":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"z":{"docs":{},"e":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"e":{"docs":{},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":2.5070093457943927}}},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"o":{"docs":{},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}},"e":{"docs":{},"p":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"s":{"docs":{},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},"b":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}},"f":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"a":{"docs":{},"v":{"docs":{},"i":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"i":{"docs":{},"s":{"docs":{},"c":{"docs":{},"u":{"docs":{},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.018691588785046728}}}}}},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"b":{"docs":{},"u":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"e":{"docs":{},"v":{"docs":{},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},")":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},";":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"x":{"docs":{},"e":{"docs":{},"r":{"docs":{},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.031746031746031744}}}},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.021164021164021163},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"n":{"docs":{},"v":{"docs":{},"i":{"docs":{},"r":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}}},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}},"[":{"docs":{},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"r":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125}}}},"docs":{}}}}}}}}}}},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"q":{"docs":{},"u":{"docs":{},"a":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":5.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"l":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"t":{"docs":{},"c":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"f":{"docs":{},"i":{"docs":{},"e":{"docs":{},"l":{"docs":{},"d":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"n":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}},"i":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"o":{"docs":{},"l":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"s":{"docs":{},":":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.015625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":3.386243386243386},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":2.526578073089701},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}},":":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.014018691588785047}}}}},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"s":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"e":{"docs":{},"e":{"docs":{},"d":{"docs":{},"b":{"docs":{},"a":{"docs":{},"c":{"docs":{},"k":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"g":{"docs":{},"o":{"docs":{},"n":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"o":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581}},"\"":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"n":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"/":{"docs":{},"b":{"docs":{},"a":{"docs":{},"d":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"u":{"docs":{},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}},",":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}}},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"docs":{}}}},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.026455026455026454},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"t":{"docs":{},"=":{"docs":{},"‚àë":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"‚àû":{"docs":{},"Œ≥":{"docs":{},"k":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"docs":{}}}}}}}}}},"docs":{}}}},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"‚ãØ":{"docs":{},"=":{"docs":{},"‚àë":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"‚àû":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}},"docs":{}}}},"Œ≥":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"‚ãØ":{"docs":{},"=":{"docs":{},"‚àë":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"‚àû":{"docs":{},"Œ≥":{"docs":{},"k":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"‚â§":{"docs":{},"‚àë":{"docs":{},"t":{"docs":{},"=":{"0":{"docs":{},"‚àû":{"docs":{},"Œ≥":{"docs":{},"t":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"=":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"1":{"docs":{},"‚àí":{"docs":{},"Œ≥":{"docs":{},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}},"docs":{}}}}},"g":{"docs":{},"_":{"docs":{},"t":{"docs":{},"g":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"h":{"docs":{},"e":{"docs":{},"l":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"r":{"docs":{},"e":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"o":{"docs":{},"l":{"docs":{},"d":{"docs":{},"s":{"docs":{},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"r":{"docs":{},"i":{"docs":{},"z":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}},"w":{"docs":{},"e":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"y":{"docs":{},"p":{"docs":{},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"s":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"i":{"docs":{},"'":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"n":{"docs":{},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"i":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"r":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"t":{"docs":{"./":{"ref":"./","tf":10.015873015873016},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"u":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"i":{"docs":{},"c":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"f":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"t":{"docs":{},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"d":{"docs":{},"e":{"docs":{},"a":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.07936507936507936},"the-basics/types-of-ml.html":{"ref":"the-basics/types-of-ml.html","tf":3.6666666666666665},"the-basics/ml-pipeline.html":{"ref":"the-basics/ml-pipeline.html","tf":3.6666666666666665},"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":5.2}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"?":{"docs":{"the-basics/what-is-ml.html":{"ref":"the-basics/what-is-ml.html","tf":5.5}}}}}}}},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}},"t":{"docs":{},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"n":{"docs":{},"g":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"i":{"docs":{},"z":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"'":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"v":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"m":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.047619047619047616},"the-basics/what-is-ml.html":{"ref":"the-basics/what-is-ml.html","tf":5.5},"the-basics/types-of-ml.html":{"ref":"the-basics/types-of-ml.html","tf":3.6666666666666665},"the-basics/ml-pipeline.html":{"ref":"the-basics/ml-pipeline.html","tf":3.6666666666666665},"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667}}}}}},"k":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"x":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"i":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},"u":{"docs":{},"m":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}},"r":{"docs":{},"k":{"docs":{},"o":{"docs":{},"v":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":2.5093457943925235}}}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"i":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"p":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"e":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"a":{"docs":{},"s":{"docs":{},"u":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"m":{"docs":{},"o":{"docs":{},"r":{"docs":{},"y":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"d":{"docs":{},"p":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"!":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{},"p":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"i":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},":":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"r":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"y":{"docs":{},"o":{"docs":{},"p":{"docs":{},"i":{"docs":{},"c":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}}},"x":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"o":{"docs":{},"t":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"w":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}}}}}}}}},"u":{"docs":{},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":3.386243386243386},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":2.5299003322259135}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}},"y":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},":":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581}}}}}}},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{},"t":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"v":{"docs":{},"i":{"docs":{},"o":{"docs":{},"u":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"i":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.031746031746031744},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":2.5023364485981308}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"b":{"docs":{},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.011682242990654205}}}}},"l":{"docs":{},"e":{"docs":{},"m":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}}},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"i":{"docs":{},"p":{"docs":{},"e":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"the-basics/ml-pipeline.html":{"ref":"the-basics/ml-pipeline.html","tf":3.6666666666666665}}}}}}},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}},"e":{"docs":{},"r":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"[":{"docs":{},"a":{"docs":{},"_":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"|":{"docs":{},"s":{"docs":{},"_":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"a":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"s":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}},"docs":{}}}}},"t":{"docs":{},"+":{"1":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"]":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚à£":{"docs":{},"s":{"1":{"docs":{},",":{"docs":{},"‚ãØ":{"docs":{},"s":{"docs":{},"t":{"docs":{},"]":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}},"a":{"docs":{},"i":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"u":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"\\":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"{":{"docs":{},"p":{"docs":{},"}":{"docs":{},"p":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}},"_":{"docs":{},"{":{"1":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{},"n":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"docs":{},"n":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{},"n":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"l":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"y":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"s":{"docs":{},",":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"=":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{},"i":{"docs":{},"d":{"docs":{},"l":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"t":{"docs":{},"e":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"e":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"s":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"l":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}},"s":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"c":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"c":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":5.133333333333334}}}}}}}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.02336448598130841}},".":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667}}},":":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}},"?":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}},"l":{"docs":{},"a":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},"h":{"docs":{},"i":{"docs":{},"p":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.021164021164021163},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},"‚Äî":{"docs":{},"w":{"docs":{},"h":{"docs":{},"i":{"docs":{},"c":{"docs":{},"h":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"o":{"docs":{},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{},"l":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"w":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.009345794392523364}}}},"2":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"3":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125}}}},"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"g":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"‚àë":{"docs":{},"‚àû":{"docs":{},"‚Äã":{"docs":{},"Œ≥":{"docs":{},"k":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}},"docs":{}}}}}}}}}}}},"docs":{}}},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"‚ãØ":{"docs":{},"=":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"‚àë":{"docs":{},"‚àû":{"docs":{},"‚Äã":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"s":{"docs":{},"a":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"‚àë":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"v":{"docs":{},"‚àó":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}}},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"‚àë":{"docs":{},"‚Äã":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"v":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}},"=":{"docs":{},"e":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}},"docs":{}}}}}}}}},"t":{"docs":{},"r":{"docs":{},"_":{"docs":{},"t":{"docs":{},"r":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"s":{"0":{"docs":{},"‚Üí":{"docs":{},"a":{"0":{"docs":{},"s":{"1":{"docs":{},"‚Üí":{"docs":{},"a":{"1":{"docs":{},"s":{"2":{"docs":{},"‚Üí":{"docs":{},"a":{"2":{"docs":{},"‚ãØ":{"docs":{},"s":{"docs":{},"_":{"0":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{}}}}},"docs":{}}}},"docs":{}}},"docs":{}}}},"docs":{}}},"docs":{}}}},"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"c":{"docs":{},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"a":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"e":{"docs":{},"a":{"docs":{},"m":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"e":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"m":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}},"k":{"docs":{},"i":{"docs":{},"l":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"l":{"docs":{},"e":{"docs":{},"w":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"o":{"docs":{},"m":{"docs":{},"e":{"docs":{},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"f":{"docs":{},"t":{"docs":{},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{"the-basics/software-tools.html":{"ref":"the-basics/software-tools.html","tf":5.5}}}}}}},"l":{"docs":{},"v":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}},"]":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125}},"v":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"[":{"docs":{},"g":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}},"_":{"1":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"2":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0234375},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"]":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"‚à£":{"docs":{},"s":{"1":{"docs":{},"‚Äã":{"docs":{},",":{"docs":{},"‚ãØ":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"docs":{}}}}},"docs":{}}}}}}}}}}}}}},"docs":{}}}}}}}}},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"y":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"i":{"docs":{},"l":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"m":{"docs":{},"i":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"n":{"docs":{},"g":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"f":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872}}}}}},"r":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"t":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.015625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.05291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.04439252336448598}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},"s":{"docs":{},".":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"r":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}},"r":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}},"o":{"docs":{},"c":{"docs":{},"h":{"docs":{},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}},"s":{"docs":{},"_":{"docs":{},"t":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}},"u":{"docs":{},"m":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"c":{"docs":{},"c":{"docs":{},"e":{"docs":{},"e":{"docs":{},"d":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}}}},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}},"‚àà":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"œÄ":{"docs":{},"‚â•":{"docs":{},"œÄ":{"docs":{},"‚Ä≤":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"'":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},")":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"‚Ä≤":{"docs":{},"s":{"docs":{},"'":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}}},"‚àà":{"docs":{},"s":{"docs":{},"s":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"o":{"docs":{},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"o":{"docs":{},"s":{"docs":{},"e":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"f":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"1":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"docs":{}}}}}}}}},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"m":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"a":{"docs":{},"t":{"docs":{},"'":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"i":{"docs":{},"n":{"docs":{},"k":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"o":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}},"o":{"docs":{},"l":{"docs":{"the-basics/software-tools.html":{"ref":"the-basics/software-tools.html","tf":5.5}}}},"p":{"docs":{},"=":{"docs":{},"f":{"docs":{},"r":{"docs":{},"o":{"docs":{},"m":{"docs":{},"[":{"docs":{},"p":{"1":{"1":{"docs":{},"‚ãØ":{"docs":{},"p":{"1":{"docs":{},"n":{"docs":{},"‚ãÆ":{"docs":{},"p":{"docs":{},"n":{"1":{"docs":{},"‚ãØ":{"docs":{},"p":{"docs":{},"n":{"docs":{},"n":{"docs":{},"]":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}},"docs":{}}}}}},"docs":{}}}},"docs":{}},"docs":{}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}}}}}}}},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{"the-basics/types-of-ml.html":{"ref":"the-basics/types-of-ml.html","tf":3.6666666666666665}}}}},"a":{"docs":{},"k":{"docs":{},"e":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"p":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.014018691588785047}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}}}},"w":{"docs":{},"o":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"t":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},":":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},",":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{},"=":{"docs":{},"p":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"t":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}}}}}}}}}},"+":{"1":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{}}}},"docs":{}}}},"docs":{}},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"d":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}},"u":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.015873015873015872},"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"c":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"p":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"t":{"docs":{},"i":{"docs":{},"l":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"n":{"docs":{},"b":{"docs":{},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"d":{"docs":{},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"c":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.007009345794392523}}}}}}}}}}}}},"w":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"y":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}},"i":{"docs":{},"t":{"docs":{},".":{"docs":{},".":{"docs":{},".":{"docs":{},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}},"h":{"docs":{},"o":{"docs":{},"'":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"e":{"docs":{},"'":{"docs":{},"r":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"l":{"docs":{},"l":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}},"l":{"docs":{},"l":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"o":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"w":{"docs":{},"!":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"r":{"docs":{},"d":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"?":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"y":{"docs":{},"o":{"docs":{},"u":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.015873015873015872}}}}},"i":{"docs":{},"e":{"docs":{},"l":{"docs":{},"d":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/overview.html":{"ref":"reinforcement-learning/overview.html","tf":0.06666666666666667}}}}}},"n":{"docs":{},"e":{"docs":{},".":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"c":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":2.553156146179402},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}},"u":{"docs":{},"t":{"docs":{},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}},"&":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.014018691588785047}},"=":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125}}},"\\":{"docs":{},"t":{"docs":{},"e":{"docs":{},"x":{"docs":{},"t":{"docs":{},"{":{"docs":{},"i":{"docs":{},"f":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"w":{"docs":{},"i":{"docs":{},"s":{"docs":{},"e":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"g":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},")":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"\\":{"docs":{},"b":{"docs":{},"e":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"{":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{},"}":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}}}},"docs":{}}}}}}}}}}}}}}},"docs":{}}}}}}},"docs":{}}}}}}}}}}}}}}},"+":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.046875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.009345794392523364}}},"=":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.05078125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.042328042328042326},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.03322259136212625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.03271028037383177}},"e":{"docs":{},"[":{"docs":{},"r":{"docs":{},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}}}}}}}}},"\\":{"docs":{},"\\":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.009345794392523364}}},"c":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.011682242990654205}},"s":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"s":{"0":{"docs":{},"‚Äã":{"docs":{},"a":{"0":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{},"s":{"1":{"docs":{},"‚Äã":{"docs":{},"a":{"1":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{},"s":{"2":{"docs":{},"‚Äã":{"docs":{},"a":{"2":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{},"‚ãØ":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"docs":{}}}},"docs":{}}}}},"docs":{}}}},"docs":{}}}}},"docs":{}}}},"docs":{}}}}}}},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"{":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{},"}":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"[":{"docs":{},"g":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"2":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"3":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"‚ãØ":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"(":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"3":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"‚ãØ":{"docs":{},")":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"g":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},")":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}},"docs":{}}}}},"docs":{}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"c":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"s":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"b":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{},"}":{"docs":{},"t":{"docs":{},"o":{"docs":{},"p":{"docs":{},"=":{"docs":{},"f":{"docs":{},"r":{"docs":{},"o":{"docs":{},"m":{"docs":{},"‚é£":{"docs":{},"‚é°":{"docs":{},"‚Äã":{"docs":{},"p":{"1":{"1":{"docs":{},"‚Äã":{"docs":{},"p":{"docs":{},"n":{"1":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{},"‚ãØ":{"docs":{},"‚ãÆ":{"docs":{},"‚ãØ":{"docs":{},"‚Äã":{"docs":{},"p":{"1":{"docs":{},"n":{"docs":{},"‚Äã":{"docs":{},"p":{"docs":{},"n":{"docs":{},"n":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{},"‚é¶":{"docs":{},"‚é§":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}},"docs":{}}}}}}}}},"docs":{}}}}},"docs":{}},"docs":{}}}}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.03515625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"^":{"2":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"docs":{},"k":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"}":{"docs":{},"g":{"docs":{},"t":{"docs":{},"‚Äã":{"docs":{},"=":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"‚ãØ":{"docs":{},"=":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"‚àë":{"docs":{},"‚àû":{"docs":{},"‚Äã":{"docs":{},"Œ≥":{"docs":{},"k":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"docs":{},"k":{"docs":{},"+":{"1":{"docs":{},"‚Äã":{"docs":{},"‚â§":{"docs":{},"t":{"docs":{},"=":{"0":{"docs":{},"‚àë":{"docs":{},"‚àû":{"docs":{},"‚Äã":{"docs":{},"Œ≥":{"docs":{},"t":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Äã":{"docs":{},"=":{"1":{"docs":{},"‚àí":{"docs":{},"Œ≥":{"docs":{},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"docs":{}}}}}}}}}}}}},"docs":{}}}}}},"docs":{}}}}}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}},"e":{"docs":{},"q":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.016611295681063124},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"{":{"docs":{},"p":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"_":{"docs":{},"{":{"docs":{},"s":{"docs":{},"s":{"docs":{},"'":{"docs":{},"}":{"docs":{},"^":{"docs":{},"a":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}}},",":{"docs":{},"s":{"docs":{},"'":{"docs":{},"}":{"docs":{},"^":{"docs":{},"a":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}},"r":{"docs":{},"}":{"docs":{},"_":{"docs":{},"s":{"docs":{},"^":{"docs":{},"a":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"x":{"docs":{},"^":{"docs":{},"a":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}},"x":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"a":{"docs":{},"'":{"docs":{},"|":{"docs":{},"s":{"docs":{},"'":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}},"|":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125}}}}}},"s":{"docs":{},")":{"docs":{},"a":{"docs":{},"=":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}},"'":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"a":{"docs":{},"|":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"œÄ":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"‚â•":{"docs":{},"œÄ":{"docs":{},",":{"docs":{},"‚àÄ":{"docs":{},"œÄ":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"_":{"docs":{},"{":{"docs":{},"a":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}},"\\":{"docs":{},"i":{"docs":{},"n":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}},"s":{"docs":{},"'":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"s":{"docs":{},"p":{"docs":{},"l":{"docs":{},"a":{"docs":{},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"y":{"docs":{},"l":{"docs":{},"e":{"docs":{},"\\":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"_":{"docs":{},"{":{"docs":{},"k":{"docs":{},"=":{"0":{"docs":{},"}":{"docs":{},"^":{"docs":{},"\\":{"docs":{},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}}}}}}}}},"docs":{}}},"t":{"docs":{},"=":{"0":{"docs":{},"}":{"docs":{},"^":{"docs":{},"\\":{"docs":{},"i":{"docs":{},"n":{"docs":{},"f":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"docs":{}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}},"b":{"docs":{},"e":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"{":{"docs":{},"c":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"s":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"b":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"{":{"docs":{},"r":{"docs":{},"_":{"docs":{},"{":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{},"}":{"docs":{},"{":{"1":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}},"docs":{}}}}}}}}}}}}}}},"t":{"docs":{},"e":{"docs":{},"x":{"docs":{},"t":{"docs":{},"{":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.016611295681063124}},"f":{"docs":{},"r":{"docs":{},"o":{"docs":{},"m":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}},"t":{"docs":{},"o":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{},"{":{"docs":{},"a":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"'":{"docs":{},"}":{"docs":{},"{":{"docs":{},"\\":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"}":{"docs":{},"{":{"docs":{},"\\":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"q":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"v":{"docs":{},"d":{"docs":{},"o":{"docs":{},"t":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"x":{"docs":{},"r":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"w":{"docs":{},"{":{"docs":{},"a":{"docs":{},"_":{"0":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"1":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"2":{"docs":{},"}":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}},"docs":{}}}}}}}}}}}}}}}},"q":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01171875},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.015873015873015872},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"a":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"docs":{}}}}}}}}}},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.009966777408637873}}}}}}},"q":{"docs":{},"q":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},")":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{},"q":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}}}}},"docs":{}}}}}}}}}},"docs":{}}}},"g":{"docs":{},"t":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"]":{"docs":{},"q":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"‚àë":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}},"‚àë":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"a":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},",":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"‚àë":{"docs":{},"‚Äã":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"a":{"docs":{},"‚àë":{"docs":{},"‚Äã":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},",":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"q":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}},"‚àó":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"=":{"docs":{},"q":{"docs":{},"‚àó":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"q":{"docs":{},"_":{"docs":{},"{":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"_":{"docs":{},"*":{"docs":{},"}":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"‚àó":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"0":{"docs":{},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"w":{"docs":{},"i":{"docs":{},"s":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}},"docs":{},"=":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Å°":{"docs":{},"œÄ":{"docs":{},"q":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"‚àë":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Å°":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}},"x":{"docs":{},"a":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"‚àë":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"v":{"docs":{},"‚àó":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}},"‚Ä≤":{"docs":{},",":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{},"q":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}}}}}}},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"=":{"docs":{},"r":{"docs":{},"x":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"‚àë":{"docs":{},"‚Äã":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"v":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"w":{"docs":{},"i":{"docs":{},"s":{"docs":{},"e":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}},"‚Ä≤":{"docs":{},",":{"docs":{},"a":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}},"v":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125}}}},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0078125},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},"_":{"docs":{},"{":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"}":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"docs":{}}}}}}}}}},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},"'":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}}},")":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"v":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}},"{":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"'":{"docs":{},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.01953125},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":3.3703703703703702},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":2.5332225913621262},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},"e":{"docs":{},",":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"s":{"docs":{},".":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}},"v":{"docs":{},"v":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"[":{"docs":{},"g":{"docs":{},"t":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"+":{"docs":{},"Œ≥":{"2":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"3":{"docs":{},"+":{"docs":{},"‚ãØ":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"=":{"docs":{},"e":{"docs":{},"œÄ":{"docs":{},"[":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"1":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"(":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"2":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"r":{"docs":{},"t":{"docs":{},"+":{"3":{"docs":{},"+":{"docs":{},"‚ãØ":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}},"docs":{}}}}}}},"docs":{}}}}}}}},"docs":{}}}}}}}}}}}}}}}}},"docs":{}}}}},"docs":{}}}},"docs":{}}}}}}},"docs":{}}}}}}}},"v":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}},"‚àë":{"docs":{},"a":{"docs":{},"‚àà":{"docs":{},"a":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"(":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"‚àë":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}},"q":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},",":{"docs":{},"a":{"docs":{},")":{"docs":{},"v":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}},"‚â•":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"‚Ä≤":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},",":{"docs":{},"‚àÄ":{"docs":{},"s":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"‚àà":{"docs":{},"a":{"docs":{},"‚àë":{"docs":{},"‚Äã":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"(":{"docs":{},"r":{"docs":{},"s":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"+":{"docs":{},"Œ≥":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"‚àà":{"docs":{},"s":{"docs":{},"‚àë":{"docs":{},"‚Äã":{"docs":{},"p":{"docs":{},"s":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},"a":{"docs":{},"‚Äã":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},"‚Ä≤":{"docs":{},")":{"docs":{},")":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"‚â•":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"‚Ä≤":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},",":{"docs":{},"‚àÄ":{"docs":{},"s":{"docs":{},"‚àà":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}},"v":{"docs":{},"_":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}},"‚àó":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"v":{"docs":{},"‚àó":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"v":{"docs":{},"_":{"docs":{},"{":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"_":{"docs":{},"*":{"docs":{},"}":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}}}}}}}},"‚àó":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Å°":{"docs":{},"a":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.006644518272425249}}},"œÄ":{"docs":{},"v":{"docs":{},"œÄ":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}}}},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"v":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}}}}}},"v":{"docs":{},"_":{"docs":{},"*":{"docs":{},"v":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{},",":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}},":":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"‚Äã":{"docs":{},"(":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{},"x":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"i":{"docs":{},"e":{"docs":{},"w":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"|":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.0234375},"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.010582010582010581},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.011682242990654205}}},"\ud83d":{"docs":{},"\udca1":{"docs":{"reinforcement-learning/bellman-eqn.html":{"ref":"reinforcement-learning/bellman-eqn.html","tf":0.00390625},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247},"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.004672897196261682}}}},"\"":{"docs":{},"h":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}},"l":{"docs":{},"i":{"docs":{},"f":{"docs":{},"e":{"docs":{},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{},"\"":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}}},"[":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"k":{"docs":{},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}},"n":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"e":{"docs":{},"e":{"docs":{},"p":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}},"œÄ":{"docs":{},"(":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"p":{"docs":{},"[":{"docs":{},"a":{"docs":{},"t":{"docs":{},"=":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},"t":{"docs":{},"=":{"docs":{},"s":{"docs":{},"]":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"(":{"docs":{},"a":{"docs":{},"|":{"docs":{},"s":{"docs":{},")":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}}}}}}}}}}}}}}}}}}}},":":{"docs":{},"s":{"docs":{},"‚Üí":{"docs":{},"a":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},":":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}}}}}},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"œÄ":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.021164021164021163},"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"‚àó":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"^":{"docs":{},"*":{"docs":{},"œÄ":{"docs":{},"‚àó":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}}}}},"_":{"docs":{},"*":{"docs":{},"œÄ":{"docs":{},"‚àó":{"docs":{},"‚Äã":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}},"(":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"{":{"1":{"docs":{},"i":{"docs":{},"f":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}},"docs":{}}}}}}}},"‚Äã":{"docs":{},"(":{"docs":{},"a":{"docs":{},"‚à£":{"docs":{},"s":{"docs":{},")":{"docs":{},"=":{"docs":{},"‚é©":{"docs":{},"‚é®":{"docs":{},"‚éß":{"docs":{},"‚Äã":{"1":{"0":{"docs":{},"‚Äã":{"docs":{},"i":{"docs":{},"f":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"docs":{}},"docs":{}}}}}}}}}}}},"‚â•":{"docs":{},"œÄ":{"docs":{},",":{"docs":{},"‚àÄ":{"docs":{},"œÄ":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"_":{"docs":{},"*":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}}}}}},"‚Ä≤":{"docs":{},"\\":{"docs":{},"p":{"docs":{},"i":{"docs":{},"'":{"docs":{},"œÄ":{"docs":{},"‚Ä≤":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}}}}},"‚â•":{"docs":{},"œÄ":{"docs":{},"‚Ä≤":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"‚Ü©":{"docs":{"reinforcement-learning/policies-values.html":{"ref":"reinforcement-learning/policies-values.html","tf":0.005291005291005291}}},"}":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.016611295681063124}},"q":{"docs":{},"_":{"docs":{},"*":{"docs":{"reinforcement-learning/optimality.html":{"ref":"reinforcement-learning/optimality.html","tf":0.0033222591362126247}}}}}},"Œ≥":{"docs":{},"\\":{"docs":{},"g":{"docs":{},"a":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"Œ≥":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}},",":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}}}}}}}},"‚Üí":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.018691588785046728}}},"\ud83c":{"docs":{},"\udfb2":{"docs":{"reinforcement-learning/mdp.html":{"ref":"reinforcement-learning/mdp.html","tf":0.002336448598130841}}}}},"length":788},"corpusTokens":["\"how","\"lifetime\"","&","&=","&\\text{if","&\\text{otherwise}","(","(click","(each","(immediate)","(knowledg","(mdps)","(r_{t+2}","(s),","(s)v‚àó‚Äã(s)=œÄmax‚ÄãvœÄ‚Äã(s)","(s,","(Œ≥\\gammaŒ≥",")",")‚à£st=s]=eœÄ[rt+1+Œ≥gt+1‚à£st=s]=eœÄ[rt+1+Œ≥vœÄ(st+1)‚à£st=s]\\begin{align}","+","0","0,","1","1)","1.","188:","2022","61a).","8","=","=e[r_{t+1}","[","\\\\","\\begin{bmatrix}","\\begin{cases}","\\cdot","\\cdots)","\\cdotss0‚Äãa0‚Äã‚Äãs1‚Äãa1‚Äã‚Äãs2‚Äãa2‚Äã‚Äã‚ãØ","\\displaystyle\\sum_{k=0}^\\infti","\\displaystyle\\sum_{t=0}^\\infti","\\end{align}vœÄ‚Äã(s)‚Äã=eœÄ‚Äã[gt‚Äã‚à£st‚Äã=s]=eœÄ‚Äã[rt+1‚Äã+Œ≥rt+2‚Äã+Œ≥2rt+3‚Äã+‚ãØ‚à£st‚Äã=s]=eœÄ‚Äã[rt+1‚Äã+Œ≥(rt+2‚Äã+Œ≥rt+3‚Äã+‚ãØ)‚à£st‚Äã=s]=eœÄ‚Äã[rt+1‚Äã+Œ≥gt+1‚Äã‚à£st‚Äã=s]=eœÄ‚Äã[rt+1‚Äã+Œ≥vœÄ‚Äã(st+1‚Äã)‚à£st‚Äã=s]‚Äã‚Äã","\\end{bmatrix}top=from‚é£‚é°‚Äãp11‚Äãpn1‚Äã‚Äã‚ãØ‚ãÆ‚ãØ‚Äãp1n‚Äãpnn‚Äã‚Äã‚é¶‚é§‚Äã","\\end{cases}","\\foral","\\frac{r_{max}}{1","\\gamma","\\gamma^2","\\gamma^k","\\gamma^t","\\gamma}gt‚Äã=rt+1‚Äã+Œ≥rt+1‚Äã+‚ãØ=k=0‚àë‚àû‚ÄãŒ≥krt+k+1‚Äã‚â§t=0‚àë‚àû‚ÄãŒ≥trmax‚Äã=1‚àíŒ≥rmax‚Äã‚Äã","\\geq","\\in","\\leq","\\mathcal{p}","\\mathcal{p}_{s,s'}^a","\\mathcal{p}_{ss'}^a","\\mathcal{r}_s^a","\\mathcal{r}_x^a","\\max_\\pi","\\pi'","\\pi(a'|s')","\\pi(a|s)","\\pi(s)a=œÄ(s)","\\pi,","\\pi_*(a|s)","\\piœÄ‚àó‚Äã‚â•œÄ,‚àÄœÄ","\\rightarrow","\\sum_{a","\\sum_{a'","\\sum_{a\\in","\\sum_{s'","\\text{","\\text{from}","\\text{to}","\\underset{a","\\underset{a'}{\\max}","\\underset{a}{\\max}","\\vdot","\\xrightarrow{a_0}","\\xrightarrow{a_1}","\\xrightarrow{a_2}","a')","a')q‚àó‚Äã(s,a)=rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚Äãa‚Ä≤max‚Äã","a)","a)qœÄ‚àó‚Äã‚Äã(s,a)=q‚àó‚Äã(s,a)","a)q‚àó‚Äã(s,a)","a)q‚àó‚Äã(s,a)=œÄmax‚ÄãqœÄ‚Äã(s,a)","a)vœÄ‚Äã(s)=a‚ààa‚àë‚ÄãœÄ(a‚à£s)qœÄ‚Äã(s,a)","a)v‚àó‚Äã(s)=amax‚Äã","a,","a=arg‚Å°max‚Å°a‚ààa","a=a‚ààaargmax‚Äã","a=œÄ(s)a","a]ps,s‚Ä≤a‚Äã=p[st+1‚Äã=s‚Ä≤‚à£st‚Äã=s,at‚Äã=a]","a]qœÄ‚Äã(s,a)=eœÄ‚Äã[gt‚Äã‚à£st‚Äã=s,at‚Äã=a]","a]qœÄ‚Äã(s,a)=eœÄ‚Äã[rt+1‚Äã+Œ≥qœÄ‚Äã(st+1‚Äã,at+1‚Äã)‚à£st‚Äã=s,at‚Äã=a]","a]rsa‚Äã=e[rt+1‚Äã‚à£st‚Äã=s,at‚Äã=a]","a_t","a_{t+1})","aaa","aa‚ààa","above,","achiev","action","action,","action.","adapt","affect","after.","agent","agent'","ahead","algorithm","along","alway","and,","animal/human","another.","appli","area","arriv","arrives,","articl","artifici","as:","associ","ata_tat‚Äã.","avoid","a}","a}{\\arg\\max}","aœÄ:s‚Üía,","a‚ààaa","basic","befor","begin","behavior","behavior.","bellman","berkeley'","best","better","between","both","broad,","but,","call","card","choos","close","closer","combin","compar","compil","compon","condition","consid","construct","conveni","convers","cs","cumul","current","cyclic","david","deal","decid","decis","decompos","deep","defin","degre","depend","describ","determinist","differ","discount","discret","discuss","distribut","do","dozen","e_\\pi","e_\\pi[g_t","e_\\pi[r_{t+1}","each","effect","end","environ","equal","equat","equation.","estim","etc.","evalu","evaluation).","evaluation;","evolv","exercis","exist","expect","experi","express","factor","factors.","far","feedback","field,","find","finit","first","follow","follows:","fulli","function","function,","function.","function:","futur","g_t","g_{t+1}","game","given","goal","gone","good","good\"","goodness/bad","great","greater","greatest","grow","gt=rt+1+rt+1+‚ãØ=‚àëk=0‚àûrt+k+1g_t","gt=rt+1+Œ≥rt+1+‚ãØ=‚àëk=0‚àûŒ≥krt+k+1‚â§‚àët=0‚àûŒ≥trmax=rmax1‚àíŒ≥g_t","gt=‚àëk=0‚àûŒ≥krt+k+1g_t","gtg_tgt‚Äã","guidebook","guidebook,","guidelin","help","here.","holds:","horizon","however,","hypothesi","i'v","idea","immedi","independ","independent.","indic","infinit","inher","insert","intelligence,","intimid","introduc","introduct","intuitively,","it'","it.","keep","know","known","last","lead","learn","learning?","let'","limit","live","lizard'","longer","look","machin","make","maker","map","markov","mathemat","mathematically,","matrix","matrix.","max","maxim","maximum","mdp","mdp!","mdp,","mdp.","me)","mean","measur","memoryless","memoryless,","model:","more","multipl","multipli","myopic","new","newcomer.","next","nondeterminist","note","notion","now","now,","number","onc","one.","one:","optim","optimality,","order","outlin","output","over","p(s'","p[a_t=a|s_t=s]œÄ(a‚à£s)=p[at‚Äã=a‚à£st‚Äã=s]","p[s_{t+1}","p[st+1‚à£st]=p[st+1‚à£s1,‚ãØst]p[s_{t+1}","p\\mathcal{p}p","p_{11}","p_{1n}","p_{n1}","p_{nn}","pair","particular","past","perform","pictori","pipelin","place","player","poker","polici","policies.","policies:","policy,","policy:","possibl","predict","prefer","present","present.","pretti","previou","prior","probabl","problem","problems,","process","processes,","processes.","program","properti","ps,s‚Ä≤a=p[st+1=s‚Ä≤‚à£st=s,at=a]","python","q","q_*(s',","q_*(s,","q_\\pi(s',","q_\\pi(s,","q_\\pi(s,a)","q_\\pi(s_{t+1},","qqq","qualiti","question.","qœÄ(s,a)=eœÄ[gt‚à£st=s,at=a]q_\\pi(s,","qœÄ(s,a)=eœÄ[rt+1+Œ≥qœÄ(st+1,at+1)‚à£st=s,at=a]q_\\pi(s,","qœÄ(s,a)=rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤avœÄ(s‚Ä≤)","qœÄ(s,a)=rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤a‚àëa‚Ä≤‚ààaœÄ(a‚Ä≤‚à£s‚Ä≤)qœÄ(s‚Ä≤,a‚Ä≤)","qœÄq_\\piqœÄ‚Äã","qœÄ‚Äã(s,a)=rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚Äãa‚Ä≤‚ààa‚àë‚ÄãœÄ(a‚Ä≤‚à£s‚Ä≤)qœÄ‚Äã(s‚Ä≤,a‚Ä≤)","qœÄ‚Äã(s,a)=rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚ÄãvœÄ‚Äã(s‚Ä≤)","qœÄ‚àó(s,a)=q‚àó(s,a)q_{\\pi_*}(s,","q‚àó(s,a)0otherwis","q‚àó(s,a)=max‚Å°œÄqœÄ(s,a)q_*(s,a)","q‚àó(s,a)=rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤amax‚Å°a‚Ä≤","q‚àó(s,a)=rxa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤av‚àó(s‚Ä≤)","q‚àó(s,a)q_*(s,","q‚àó(s,a)v_*(s)","q‚àó(s‚Ä≤,a‚Ä≤)q_*(s,","q‚àóq_*q‚àó‚Äã","q‚àóq_*q‚àó‚Äã,","q‚àó‚Äã(s,a)","q‚àó‚Äã(s,a)=rxa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚Äãv‚àó‚Äã(s‚Ä≤)","q‚àó‚Äã(s,a)otherwise‚Äã","q‚àó‚Äã(s‚Ä≤,a‚Ä≤)","r_{max}","r_{t+1}","r_{t+2}","r_{t+3}","r_{t+k+1}","r_{t+k+1}gt‚Äã=k=0‚àë‚àû‚ÄãŒ≥krt+k+1‚Äã","r_{t+k+1}gt‚Äã=rt+1‚Äã+rt+1‚Äã+‚ãØ=k=0‚àë‚àû‚Äãrt+k+1‚Äã","random","rapidli","rate","rate,","realli","recal","recall,","recommend","recurr","recurs","reinforc","relat","relationship","relationship,","relationship:","repres","represent","represented,","resourc","return","return‚Äîwhich","reward","reward.","reward:","rewards,","rewards?","roughli","row","rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤av‚àó(s‚Ä≤)v_*(s)","rsa=e[rt+1‚à£st=s,at=a]","rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚Äãv‚àó‚Äã(s‚Ä≤)","rtr_trt‚Äã","s","s'","s')","s,","s0‚Üía0s1‚Üía1s2‚Üía2‚ãØs_0","s]","s]vœÄ‚Äã(s)=eœÄ‚Äã[gt‚Äã‚à£st‚Äã=s]","s_1","s_1,","s_2","s_t","s_t]","s_t]p[st+1‚Äã‚à£st‚Äã]=p[st+1‚Äã‚à£s1‚Äã,‚ãØst‚Äã]","same","saying,","scalar","scrap","seamless","search","see","seem","set","show","sight","signal","silver'","similarly,","singl","skill","slew","softwar","solv","someon","specif","spring","sss","ss‚àà","start","state","state,","state.","states.","stochast","structure.","sts_tst‚Äã","succeed","successor","sum","s}","sœÄ‚â•œÄ‚Ä≤","s‚Ä≤s's‚Ä≤","s‚ààss","t(s,a,s‚Ä≤)=p(s‚Ä≤‚à£s,a).","t(s,a,s‚Ä≤)=p(s‚Ä≤‚à£s,a)t(s,","t+1t+1t+1","take","take,","take.","termin","terminated.","that'","that,","theorem","thereafter.1","think","those","through","time","time.","timestep","togeth","tool","top=from[p11‚ãØp1n‚ãÆpn1‚ãØpnn]","total","transit","ttt","ttt,","ttt:","tutorials,","two","type","uc","unboundedly.","uncertainti","up","us","util","v_*(s')","v_*(s)vœÄ‚àó‚Äã‚Äã(s)=v‚àó‚Äã(s)","v_\\pi","v_\\pi(s')","v_\\pi(s)","v_\\pi(s_{t+1})","v_{\\pi'}","valu","value,","values.","view","vvv","vœÄ(s)=eœÄ[gt‚à£st=s]=eœÄ[rt+1+Œ≥rt+2+Œ≥2rt+3+‚ãØ‚à£st=s]=eœÄ[rt+1+Œ≥(rt+2+Œ≥rt+3+‚ãØ","vœÄ(s)=eœÄ[gt‚à£st=s]v_\\pi(s)","vœÄ(s)=‚àëa‚ààaœÄ(a‚à£s)=(rsa+Œ≥‚àës‚Ä≤‚ààspss‚Ä≤avœÄ(s‚Ä≤))","vœÄ(s)=‚àëa‚ààaœÄ(a‚à£s)qœÄ(s,a)v_\\pi(s)","vœÄ(s)‚â•vœÄ‚Ä≤(s),‚àÄs‚ààs\\pi","vœÄv_\\pivœÄ‚Äã","vœÄ‚Äã(s)=a‚ààa‚àë‚ÄãœÄ(a‚à£s)=(rsa‚Äã+Œ≥s‚Ä≤‚ààs‚àë‚Äãpss‚Ä≤a‚ÄãvœÄ‚Äã(s‚Ä≤))","vœÄ‚Äã(s)‚â•vœÄ‚Ä≤‚Äã(s),‚àÄs‚àà","vœÄ‚àó(s)=v‚àó(s)v_{\\pi_*}(s)","v‚àó(s)=max‚Å°a","v‚àó(s)=max‚Å°œÄvœÄ(s)v_*(s)","v‚àó(s)v_*(s)v‚àó‚Äã(s)","v‚àóv_*v‚àó‚Äã,","v‚àóv_*v‚àó‚Äã:","v‚àó‚Äã(s)=amax‚Äã","wait...how","want","way.","we'll","we'r","weight","well","who'","with?","wonder","wondering,","words,","wow!","yield","you.","|","}","}q_*","Œ≥\\gammaŒ≥","Œ≥\\gammaŒ≥,","œÄ(a‚à£s)=p[at=a‚à£st=s]\\pi(a|s)","œÄ:s‚Üía\\pi:","œÄ\\piœÄ","œÄ‚Ä≤\\pi'œÄ‚Ä≤","œÄ‚àó(a‚à£s)={1if","œÄ‚àó\\pi^*œÄ‚àó","œÄ‚àó\\pi_*œÄ‚àó‚Äã","œÄ‚àó‚Äã(a‚à£s)=‚é©‚é®‚éß‚Äã10‚Äãif","œÄ‚àó‚â•œÄ,‚àÄœÄ\\pi_*","œÄ‚â•œÄ‚Ä≤","‚Üí","‚Ü©","üé≤","üí°"],"pipeline":["stopWordFilter","stemmer"]},"store":{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction to Machine Learning\nMachine learning is a broad, rapidly evolving field, and this can be pretty intimidating to a newcomer. As someone who's gone through the process of learning machine learning by scrapping together dozens of tutorials, I want to make this process as seamless as possible for you. I've compiled notes from a slew of resources along with exercises to help you apply the skills you learn along the way.\nTo best maximize your experience with this guidebook, I recommend you have some prior programming experience in Python (knowledge that is roughly comparable to Berkeley's CS 61A). \nHow to use this guidebook\nInsert some guidelines here.\n"},"the-basics/overview.html":{"url":"the-basics/overview.html","title":"The Basics","keywords":"","body":""},"the-basics/what-is-ml.html":{"url":"the-basics/what-is-ml.html","title":"What is machine learning?","keywords":"","body":"What is machine learning?\n"},"the-basics/types-of-ml.html":{"url":"the-basics/types-of-ml.html","title":"Types of machine learning","keywords":"","body":"Types of machine learning\n"},"the-basics/ml-pipeline.html":{"url":"the-basics/ml-pipeline.html","title":"Machine learning pipeline","keywords":"","body":"The machine learning pipeline\n"},"the-basics/software-tools.html":{"url":"the-basics/software-tools.html","title":"Software tools","keywords":"","body":"Software tools\n"},"reinforcement-learning/overview.html":{"url":"reinforcement-learning/overview.html","title":"Reinforcement Learning","keywords":"","body":"Reinforcement learning\nReinforcement learning is an area of machine learning that deals with how agents take actions on their environments in order to maximize some reward.\n"},"reinforcement-learning/bellman-eqn.html":{"url":"reinforcement-learning/bellman-eqn.html","title":"Bellman Equation","keywords":"","body":"Bellman Equation\nLet's take a closer look at the value function that we discussed last time. We will decompose it so that we can see a recursive structure.\nvœÄ(s)=EœÄ[Gt‚à£St=s]=EœÄ[Rt+1+Œ≥Rt+2+Œ≥2Rt+3+‚ãØ‚à£St=s]=EœÄ[Rt+1+Œ≥(Rt+2+Œ≥Rt+3+‚ãØ‚Äâ)‚à£St=s]=EœÄ[Rt+1+Œ≥Gt+1‚à£St=s]=EœÄ[Rt+1+Œ≥vœÄ(St+1)‚à£St=s]\\begin{align}\nv_\\pi(s) &= E_\\pi[G_t | S_t = s] \\\\ \n&= E_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s] \\\\\n&= E_\\pi[R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) | S_t = s] \\\\\n&= E_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n&= E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\\\n\\end{align}vœÄ‚Äã(s)‚Äã=EœÄ‚Äã[Gt‚Äã‚à£St‚Äã=s]=EœÄ‚Äã[Rt+1‚Äã+Œ≥Rt+2‚Äã+Œ≥2Rt+3‚Äã+‚ãØ‚à£St‚Äã=s]=EœÄ‚Äã[Rt+1‚Äã+Œ≥(Rt+2‚Äã+Œ≥Rt+3‚Äã+‚ãØ)‚à£St‚Äã=s]=EœÄ‚Äã[Rt+1‚Äã+Œ≥Gt+1‚Äã‚à£St‚Äã=s]=EœÄ‚Äã[Rt+1‚Äã+Œ≥vœÄ‚Äã(St+1‚Äã)‚à£St‚Äã=s]‚Äã‚Äã\nSimilarly, we can do the same for the Q-function:\nqœÄ(s,a)=EœÄ[Rt+1+Œ≥qœÄ(St+1,At+1)‚à£St=s,At=a]q_\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]qœÄ‚Äã(s,a)=EœÄ‚Äã[Rt+1‚Äã+Œ≥qœÄ‚Äã(St+1‚Äã,At+1‚Äã)‚à£St‚Äã=s,At‚Äã=a]\nNow, when we consider the value function, what it is really doing is it's looking ahead from its current state and performing a weighted sum of all the all the possible actions from that state to the next one. For each of those actions that we can take, there is a Q-value associated with it. Mathematically, we can express this as: \nvœÄ(s)=‚àëa‚ààAœÄ(a‚à£s)qœÄ(s,a)v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) q_\\pi(s, a)vœÄ‚Äã(s)=a‚ààA‚àë‚ÄãœÄ(a‚à£s)qœÄ‚Äã(s,a)\n\n\n  üí° Pictorial representation of the relationship between qqq and vvv (click me)  \n\n \n\n\nNow, to find the converse relationship, we now consider that we're in a given state, and we are wondering how good it is to take a specific action. For this action, we will get an immediate reward and will then end up in a specific state as given by the state transition matrix. We now can use the value function to find the expected return from this new state. Mathematically, we can express this as:\nqœÄ(s,a)=Rsa+Œ≥‚àës‚Ä≤‚ààSPss‚Ä≤avœÄ(s‚Ä≤) q_\\pi(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a v_\\pi(s') qœÄ‚Äã(s,a)=Rsa‚Äã+Œ≥s‚Ä≤‚ààS‚àë‚ÄãPss‚Ä≤a‚ÄãvœÄ‚Äã(s‚Ä≤)\nCombining the two representations above, we find a recurrence relation for the value function and Q-function as follows:\nvœÄ(s)=‚àëa‚ààAœÄ(a‚à£s)=(Rsa+Œ≥‚àës‚Ä≤‚ààSPss‚Ä≤avœÄ(s‚Ä≤)) v_\\pi(s) = \\sum_{a\\in A} \\pi(a|s) = ( \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a v_\\pi(s') ) vœÄ‚Äã(s)=a‚ààA‚àë‚ÄãœÄ(a‚à£s)=(Rsa‚Äã+Œ≥s‚Ä≤‚ààS‚àë‚ÄãPss‚Ä≤a‚ÄãvœÄ‚Äã(s‚Ä≤))\nqœÄ(s,a)=Rsa+Œ≥‚àës‚Ä≤‚ààSPss‚Ä≤a‚àëa‚Ä≤‚ààAœÄ(a‚Ä≤‚à£s‚Ä≤)qœÄ(s‚Ä≤,a‚Ä≤) q_\\pi(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a \\sum_{a' \\in A} \\pi(a'|s') q_\\pi(s', a') qœÄ‚Äã(s,a)=Rsa‚Äã+Œ≥s‚Ä≤‚ààS‚àë‚ÄãPss‚Ä≤a‚Äãa‚Ä≤‚ààA‚àë‚ÄãœÄ(a‚Ä≤‚à£s‚Ä≤)qœÄ‚Äã(s‚Ä≤,a‚Ä≤)\n\nAdapted from UC Berkeley's CS 188: Introduction to Artificial Intelligence, Spring 2022 Note 8 and David Silver's Markov Decision Processes\n"},"reinforcement-learning/policies-values.html":{"url":"reinforcement-learning/policies-values.html","title":"Policies and Value Functions","keywords":"","body":"Policies and Value Functions\nNow, we discuss how do we measure \"how good\" it is for an agent to choose a given action in a specific state. A policy is the agent's behavior. A value function is a prediction of the future reward that is used to evaluate the goodness/badness of states.\nPolicies\nA policy, œÄ:S‚ÜíA\\pi: S \\rightarrow AœÄ:S‚ÜíA, is a function that maps each state to an action. \n\nA deterministic policy: a=œÄ(s)a = \\pi(s)a=œÄ(s) describes how to get from some state sss to some action aaa\nA stochastic policy: œÄ(a‚à£s)=P[At=a‚à£St=s]\\pi(a|s) = P[A_t=a|S_t=s]œÄ(a‚à£s)=P[At‚Äã=a‚à£St‚Äã=s] is a distribution over actions given states\n\nThe optimal policy is a policy œÄ‚àó\\pi^*œÄ‚àó that has an expected return that is greater than or equal to the expected return or utility for all states as compared to all policies.\nValue Functions\nValue functions are functions of states or of state-action pairs that estimate how good it is for an agent to be in a specific state or how good a specific action is in a given state.\nThe state-value function for policy œÄ\\piœÄ is vœÄv_\\pivœÄ‚Äã is the expected return an agent starting at state sss at time ttt that follows policy œÄ\\piœÄ thereafter.1\nvœÄ(s)=EœÄ[Gt‚à£St=s]v_\\pi(s) = E_\\pi[G_t | S_t = s]vœÄ‚Äã(s)=EœÄ‚Äã[Gt‚Äã‚à£St‚Äã=s]\nThe action-value function for policy œÄ\\piœÄ is qœÄq_\\piqœÄ‚Äã is the expected return from starting at state sss at time ttt taking action aaa that follows policy œÄ\\piœÄ after. \nqœÄ(s,a)=EœÄ[Gt‚à£St=s,At=a]q_\\pi(s, a) = E_\\pi [ G_t | S_t = s, A_t = a]qœÄ‚Äã(s,a)=EœÄ‚Äã[Gt‚Äã‚à£St‚Äã=s,At‚Äã=a]\nThis is also known as the Q-function, and the output from the function is called a Q-value, where Q represents the quality of a given action in a given state.\n\n1. Recall, Gt=‚àëk=0‚àûŒ≥kRt+k+1G_t = \\displaystyle\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}Gt‚Äã=k=0‚àë‚àû‚ÄãŒ≥kRt+k+1‚Äã ‚Ü©\n\n\nAdapted from UC Berkeley's CS 188: Introduction to Artificial Intelligence, Spring 2022 Note 8 and Deep Lizard's Policies and Value Functions Article\n"},"reinforcement-learning/optimality.html":{"url":"reinforcement-learning/optimality.html","title":"Optimal Policies and Value Functions","keywords":"","body":"Optimality\nOptimal Value Function\nThe optimal value function outlines the best possible performance in an MDP, and this is why an MDP is effectively solved when we know the optimal value function. \nThe optimal state-value function v‚àó(s)v_*(s)v‚àó‚Äã(s) is the maximum value function over all policies: \nv‚àó(s)=max‚Å°œÄvœÄ(s)v_*(s) = \\max_\\pi v_\\pi (s)v‚àó‚Äã(s)=œÄmax‚ÄãvœÄ‚Äã(s)\nThe optimal action-value function q‚àó(s,a)q_*(s, a)q‚àó‚Äã(s,a) is the maximum action-value function over all policies: \nq‚àó(s,a)=max‚Å°œÄqœÄ(s,a)q_*(s,a) = \\max_\\pi q_\\pi(s, a)q‚àó‚Äã(s,a)=œÄmax‚ÄãqœÄ‚Äã(s,a)\nOptimal Policy\nBefore we consider optimality, we will first introduce this notion of what makes a policy better than another. A policy œÄ\\piœÄ is better than a policy œÄ‚Ä≤\\pi'œÄ‚Ä≤ if the following holds: \nœÄ‚â•œÄ‚Ä≤¬†if¬†vœÄ(s)‚â•vœÄ‚Ä≤(s),‚àÄs‚ààS\\pi \\geq \\pi' \\text{ if } v_\\pi(s) \\geq v_{\\pi'} (s), \\forall s \\in SœÄ‚â•œÄ‚Ä≤¬†if¬†vœÄ‚Äã(s)‚â•vœÄ‚Ä≤‚Äã(s),‚àÄs‚ààS\n\nüí° Theorem\nFor any MDP, \n\n There exists an optimal policy œÄ‚àó\\pi_*œÄ‚àó‚Äã that is better than or equal to all other policies: œÄ‚àó‚â•œÄ,‚àÄœÄ\\pi_* \\geq \\pi, \\forall \\piœÄ‚àó‚Äã‚â•œÄ,‚àÄœÄ \n All optimal policies achieve the optimal value function, vœÄ‚àó(s)=v‚àó(s)v_{\\pi_*}(s) = v_*(s)vœÄ‚àó‚Äã‚Äã(s)=v‚àó‚Äã(s)\n All optimal policies achieve the optimal action-value function, qœÄ‚àó(s,a)=q‚àó(s,a)q_{\\pi_*}(s, a) = q_*(s, a)qœÄ‚àó‚Äã‚Äã(s,a)=q‚àó‚Äã(s,a)\n\n\nFinding an Optimal Policy\nRecall that there is always a deterministic optimal policy for any MDP. We will now describe an algorithm to find one:\nœÄ‚àó(a‚à£s)={1if¬†a=arg‚Å°max‚Å°a‚ààA¬†q‚àó(s,a)0otherwise\n\\pi_*(a|s) = \\begin{cases}\n   1 &\\text{if } a = \\underset{a \\in A}{\\arg\\max} \\text{ }q_* (s, a) \\\\\n   0 &\\text{otherwise} \n\\end{cases}\nœÄ‚àó‚Äã(a‚à£s)=‚é©‚é®‚éß‚Äã10‚Äãif¬†a=a‚ààAargmax‚Äã¬†q‚àó‚Äã(s,a)otherwise‚Äã\nYou might be wondering, wow! once we have q‚àóq_*q‚àó‚Äã, we can solve any MDP! But, wait...how do we get q‚àóq_*q‚àó‚Äã to begin with? And, that's a great question. For that, we'll introduce the Bellman Optimality Equation.\nBellman Optimality Equation\nFor v‚àóv_*v‚àó‚Äã, we define the following relationship:\nv‚àó(s)=max‚Å°a¬†q‚àó(s,a)v_*(s) = \\underset{a}{\\max} \\text{ } q_*(s, a)v‚àó‚Äã(s)=amax‚Äã¬†q‚àó‚Äã(s,a)\nIntuitively, this function is saying, at a particular state, we look ahead to all the possible actions we can possibly take, and we choose the action that yields the greatest expected return‚Äîwhich is just the max of all the Q-values. \nFor q‚àóq_*q‚àó‚Äã, we define the following relationship:\nq‚àó(s,a)=Rxa+Œ≥‚àës‚Ä≤‚ààSPss‚Ä≤av‚àó(s‚Ä≤) q_*(s, a) = \\mathcal{R}_x^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a v_*(s') q‚àó‚Äã(s,a)=Rxa‚Äã+Œ≥s‚Ä≤‚ààS‚àë‚ÄãPss‚Ä≤a‚Äãv‚àó‚Äã(s‚Ä≤)\nIntuitively, we have the immediate reward from taking the given action, and then we sum over the probabilities of starting in the current state and ending up in a different state multiplied by the value function of each ending state.\nWe can now introduce recursive relationships for both q‚àóq_*q‚àó‚Äã and v‚àóv_*v‚àó‚Äã:\nv‚àó(s)=max‚Å°a¬†Rsa+Œ≥‚àës‚Ä≤‚ààSPss‚Ä≤av‚àó(s‚Ä≤)v_*(s) = \\underset{a}{\\max} \\text{ } \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a v_*(s') v‚àó‚Äã(s)=amax‚Äã¬†Rsa‚Äã+Œ≥s‚Ä≤‚ààS‚àë‚ÄãPss‚Ä≤a‚Äãv‚àó‚Äã(s‚Ä≤)\nq‚àó(s,a)=Rsa+Œ≥‚àës‚Ä≤‚ààSPss‚Ä≤amax‚Å°a‚Ä≤¬†q‚àó(s‚Ä≤,a‚Ä≤)q_*(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}^a \\underset{a'}{\\max} \\text{ } q_*(s', a')q‚àó‚Äã(s,a)=Rsa‚Äã+Œ≥s‚Ä≤‚ààS‚àë‚ÄãPss‚Ä≤a‚Äãa‚Ä≤max‚Äã¬†q‚àó‚Äã(s‚Ä≤,a‚Ä≤)\n\nAdapted from David Silver's Markov Decision Processes\n"},"reinforcement-learning/mdp.html":{"url":"reinforcement-learning/mdp.html","title":"Markov Decision Processes (MDPs)","keywords":"","body":"Markov Decision Processes (MDPs)\nAn agent that is placed in an environment where there are multiple possible successor states from a single action in some state is a nondeterministic action. Think about a card game like poker where the randomness of dealing cards can introduce uncertainty into the succeeding actions a player might take. Problems that have an inherent degree of uncertainty are nondeterministic search problems, and they can be solved with Markov Decision Processes.\nComponents of an MDP\n\nAgent ‚Üí the decision maker\nEnvironment ‚Üí where the agent lives\nSet of States SSS ‚Üí the representation of the environment at a given time\nA start state\n1 or more terminal states ‚Üí a state where once the agent arrives, it can no longer perform any actions for more rewards\n\n\nSet of Actions AAA ‚Üí what the agent decides to do at a state\nReward RtR_tRt‚Äã ‚Üí a scalar feedback signal that indicates how well an agent is doing at timestep ttt\nState Transition Probability ‚Üí a probability function that represents the probability that an agent takes an action a‚ààAa \\in Aa‚ààA from a state s‚ààSs \\in Ss‚ààS and ends up in state s‚Ä≤s's‚Ä≤\nPs,s‚Ä≤a=P[St+1=s‚Ä≤‚à£St=s,At=a] \\mathcal{P}_{s,s'}^a = P[S_{t+1} = s' | S_t = s, A_t = a]Ps,s‚Ä≤a‚Äã=P[St+1‚Äã=s‚Ä≤‚à£St‚Äã=s,At‚Äã=a]\nState transition matrix P\\mathcal{P}P defines transition probabilities from all states sss to all successor states s‚Ä≤s's‚Ä≤ (each row sums to 1)\ntoP=from[P11‚ãØP1n‚ãÆPn1‚ãØPnn]\n    \\text{to} \\\\\n    \\mathcal{P} =     \\text{from} \n    \\begin{bmatrix}\n    \n    P_{11} & \\cdots & P_{1n} \\\\\n    & \\vdots & \\\\\n    P_{n1} & \\cdots & P_{nn} \\\\\n    \\end{bmatrix}toP=from‚é£‚é°‚ÄãP11‚ÄãPn1‚Äã‚Äã‚ãØ‚ãÆ‚ãØ‚ÄãP1n‚ÄãPnn‚Äã‚Äã‚é¶‚é§‚Äã\n\n\nReward Function ‚Üí predicts the next (immediate) reward\nRsa=E[Rt+1‚à£St=s,At=a] \\mathcal{R}_s^a =E[R_{t+1} | S_t = s, A_t = a]Rsa‚Äã=E[Rt+1‚Äã‚à£St‚Äã=s,At‚Äã=a]\n\n\nüí° Reward Hypothesis\nAll goals can be described by the maximization of expected cumulative reward \n\nReturn and Discounting\nWe can view the agent's states through discrete timesteps where at timestep ttt, an agent is at state sts_tst‚Äã and takes action ata_tat‚Äã. Starting at timestep 0, we can construct the following model:\ns0‚Üía0s1‚Üía1s2‚Üía2‚ãØs_0 \\xrightarrow{a_0} s_1 \\xrightarrow{a_1} s_2 \\xrightarrow{a_2} \\cdotss0‚Äãa0‚Äã‚Äãs1‚Äãa1‚Äã‚Äãs2‚Äãa2‚Äã‚Äã‚ãØ\nThe return GtG_tGt‚Äã is the total reward from timestep ttt:\nGt=Rt+1+Rt+1+‚ãØ=‚àëk=0‚àûRt+k+1G_t = R_{t+1} + R_{t+1} + \\cdots = \\displaystyle\\sum_{k=0}^\\infty R_{t+k+1}Gt‚Äã=Rt+1‚Äã+Rt+1‚Äã+‚ãØ=k=0‚àë‚àû‚ÄãRt+k+1‚Äã\nIt may seem like if you keep choosing the same optimal state-action-reward set that you can get your utility function to equal a value that grows unboundedly. However, we now introduce this idea of finite horizons and discount factors. A finite horizon is a \"lifetime\" for agents that limits the number of timesteps to maximize reward before the agent is terminated. A discount factor or discount rate, Œ≥\\gammaŒ≥, is a number between 0 and 1 that is the rate for which we discount future rewards (Œ≥\\gammaŒ≥ close to 0 leads to myopic evaluation; Œ≥\\gammaŒ≥ close to 1 leads to far-sighted evaluation). \nNow, we can say that our agent's goal is to maximize the total discounted reward:\nGt=Rt+1+Œ≥Rt+1+‚ãØ=‚àëk=0‚àûŒ≥kRt+k+1‚â§‚àët=0‚àûŒ≥tRmax=Rmax1‚àíŒ≥G_t = R_{t+1} + \\gamma R_{t+1} + \\cdots = \\displaystyle\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\leq \\displaystyle\\sum_{t=0}^\\infty \\gamma^t R_{max} = \\frac{R_{max}}{1-\\gamma}Gt‚Äã=Rt+1‚Äã+Œ≥Rt+1‚Äã+‚ãØ=k=0‚àë‚àû‚ÄãŒ≥kRt+k+1‚Äã‚â§t=0‚àë‚àû‚ÄãŒ≥tRmax‚Äã=1‚àíŒ≥Rmax‚Äã‚Äã\n\n\n  üé≤ Why do we use discounted rewards? (click me)  \nMathematically convenient to discount rewards, avoids infinite returns in cyclic Markov processes, uncertainty about the future may not be fully represented, animal/human behaviors show preference for immediate rewards, etc.\n\n\n\n\n\nMemoryless Property\n MDPs are memoryless, meaning that only the current state affects the future state, and the future and the past are conditionally independent. Mathematically, we can express this as: T(s,a,s‚Ä≤)=P(s‚Ä≤‚à£s,a)T(s, a, s') = P(s' | s, a) T(s,a,s‚Ä≤)=P(s‚Ä≤‚à£s,a). In other words, the probability of arriving to state s‚Ä≤s's‚Ä≤ at time t+1t+1t+1 only depends on the current state at time ttt and not any of the previous states.\n The future is independent of the past given the present.\n\nüí° Markov or Memoryless Property\nThe future is independent of the past given the present  \nP[St+1‚à£St]=P[St+1‚à£S1,‚ãØSt]P[S_{t+1} | S_t] = P[S_{t+1} | S_1, \\cdots S_t]P[St+1‚Äã‚à£St‚Äã]=P[St+1‚Äã‚à£S1‚Äã,‚ãØSt‚Äã] \n\n\nAdapted from UC Berkeley's CS 188: Introduction to Artificial Intelligence, Spring 2022 Note 8\n"}}}